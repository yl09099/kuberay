{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":""},{"location":"#kuberay","title":"KubeRay","text":"<p>KubeRay is a powerful, open-source Kubernetes operator that simplifies the deployment and management of Ray applications on Kubernetes. It offers several key components:</p> <p>KubeRay core: This is the official, fully-maintained component of KubeRay that provides three custom resource definitions, RayCluster, RayJob, and RayService. These resources are designed to help you run a wide range of workloads with ease.</p> <ul> <li> <p>RayCluster: KubeRay fully manages the lifecycle of RayCluster, including cluster creation/deletion, autoscaling, and ensuring fault tolerance.</p> </li> <li> <p>RayJob: With RayJob, KubeRay automatically creates a RayCluster and submits a job when the cluster is ready. You can also configure RayJob to automatically delete the RayCluster once the job finishes.</p> </li> <li> <p>RayService: RayService is made up of two parts: a RayCluster and a Ray Serve deployment graph. RayService offers zero-downtime upgrades for RayCluster and high availability.</p> </li> </ul> <p>Community-managed components (optional): Some components are maintained by the KubeRay community.</p> <ul> <li> <p>KubeRay APIServer: It provides a layer of simplified configuration for KubeRay resources. The KubeRay API server is used internally by some organizations to back user interfaces for KubeRay resource management.</p> </li> <li> <p>KubeRay Python client: This Python client library provides APIs to handle RayCluster from your Python application.</p> </li> <li> <p>KubeRay CLI: KubeRay CLI provides the ability to manage KubeRay resources through command-line interface.</p> </li> </ul>"},{"location":"#kuberay-ecosystem","title":"KubeRay ecosystem","text":"<ul> <li>AWS Application Load Balancer</li> <li>Nginx</li> <li>Prometheus and Grafana </li> <li>Volcano</li> <li>MCAD</li> <li>Kubeflow</li> </ul>"},{"location":"#security","title":"Security","text":"<p>Security and isolation must be enforced outside of the Ray Cluster. Restrict network access with Kubernetes or other external controls. Refer to Ray security documentation for more guidance on what controls to implement.</p> <p>Please report security issues to security@anyscale.com.</p>"},{"location":"#the-ray-docs","title":"The Ray docs","text":"<p>You can find even more information on deployments of Ray on Kubernetes at the official Ray docs.</p>"},{"location":"best-practice/worker-head-reconnection/","title":"Explanation and Best Practice for workers-head Reconnection","text":""},{"location":"best-practice/worker-head-reconnection/#problem","title":"Problem","text":"<p>For a <code>RayCluster</code> with a head and several workers, if a worker is crashed, it will be relaunched immediately and re-join the same cluster quickly; however, when the head is crashed, it will run into the issue #104 that all worker nodes are lost from the head for a long period of time. </p>"},{"location":"best-practice/worker-head-reconnection/#explanation","title":"Explanation","text":"<p>note It was an issue that only happened with old version In the Kuberay version under 0.3.0, we recommand you try the latest version  </p> <p>When the head pod was deleted, it will be recreated with a new IP by KubeRay controller\uff0cand the GCS server address is changed accordingly. The Raylets of all workers will try to get GCS address from Redis in <code>ReconnectGcsServer</code>, but the redis_clients always use the previous head IP, so they will always fail to get new GCS address. The Raylets will not exit until max retries are reached. There are two configurations determining this long delay:</p> <pre><code>/// The interval at which the gcs rpc client will check if gcs rpc server is ready.\nRAY_CONFIG(int64_t, ping_gcs_rpc_server_interval_milliseconds, 1000)\n\n/// Maximum number of times to retry ping gcs rpc server when gcs server restarts.\nRAY_CONFIG(int32_t, ping_gcs_rpc_server_max_retries, 600)\n\nhttps://github.com/ray-project/ray/blob/98be9fb5e08befbd6cac3ffbcaa477c5117b0eef/src/ray/gcs/gcs_client/gcs_client.cc#L294-L295\n</code></pre> <p>It retries 600 times and each interval is 1s, resulting in total 600s timeout, i.e. 10 min. So immediately after 10-min wait for retries, each client exits and gets restarted while connecting to the new head IP. This issue exists in stable ray versions under 1.9.1. This has been reduced to 60s in recent commit under Kuberay 0.3.0.</p>"},{"location":"best-practice/worker-head-reconnection/#solution","title":"Solution","text":"<p>We recommend using the latest version of KubeRay. After version 0.5.0, the GCS Fault-Tolerance feature is now in beta and can help resolve this reconnection issue.</p>"},{"location":"best-practice/worker-head-reconnection/#best-practice","title":"Best Practice","text":"<p>For older version (Kuberay &lt;=0.4.0, ray &lt;=2.1.0). To reduce the chances of a lost worker-head connection, there are two other options:</p> <ul> <li> <p>Make head more stable: when creating the cluster, allocate sufficient amount of resources on head pod such that it tends to be stable and not easy to crash. You can also set {\"num-cpus\": \"0\"} in \"rayStartParams\" of \"headGroupSpec\" such that Ray scheduler will skip the head node when scheduling workloads. This also helps to maintain the stability of the head. </p> </li> <li> <p>Make reconnection shorter: for version &lt;= 1.9.1, you can set this head param --system-config='{\"ping_gcs_rpc_server_max_retries\": 20}' to reduce the delay from 600s down to 20s before workers reconnect to the new head. </p> </li> </ul>"},{"location":"components/apiserver/","title":"KubeRay APIServer","text":"<p>The KubeRay APIServer provides gRPC and HTTP APIs to manage KubeRay resources.</p>"},{"location":"components/apiserver/#note","title":"Note","text":"<pre><code>The KubeRay APIServer is an optional component. It provides a layer of simplified configuration for KubeRay resources. The KubeRay API server is used internally by some organizations to back user interfaces for KubeRay resource management.\n\nThe KubeRay APIServer is community-managed and is not officially endorsed by the Ray maintainers. At this time, the only officially supported methods for\nmanaging KubeRay resources are:\n\n- Direct management of KubeRay custom resources via kubectl, kustomize, and Kubernetes language clients.\n- Helm charts.\n\nKubeRay APIServer maintainer contacts (GitHub handles):\n@Jeffwan @scarlet25151\n</code></pre>"},{"location":"components/apiserver/#installation","title":"Installation","text":""},{"location":"components/apiserver/#helm","title":"Helm","text":"<p>Make sure the version of Helm is v3+. Currently, existing CI tests are based on Helm v3.4.1 and v3.9.4.</p> <pre><code>helm version\n</code></pre>"},{"location":"components/apiserver/#install-kuberay-operator","title":"Install KubeRay Operator","text":"<ul> <li>Install a stable version via Helm repository (only supports KubeRay v0.4.0+)</li> </ul> <pre><code>  # Install the KubeRay helm repo\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\n\n  # Install KubeRay Operator v1.0.0.\nhelm install kuberay-operator kuberay/kuberay-operator --version v1.0.0\n\n  # Check the KubeRay Operator Pod in `default` namespace\nkubectl get pods\n  # NAME                                             READY   STATUS    RESTARTS   AGE\n# kuberay-operator-7456c6b69b-t6pt7                1/1     Running   0          172m\n</code></pre>"},{"location":"components/apiserver/#install-kuberay-apiserver","title":"Install KubeRay APIServer","text":"<pre><code>Please note that examples show here will only work with the nightly builds of the api-server. `v1.0.0` does not yet contain critical fixes\nto the api server that would allow Kuberay Serve endpoints to work properly\n</code></pre> <ul> <li>Install a stable version via Helm repository (only supports KubeRay v0.4.0+)</li> </ul> <pre><code># Install the KubeRay helm repo\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\n\n# Install KubeRay APIServer.\nhelm install kuberay-apiserver kuberay/kuberay-apiserver # Check the KubeRay APIServer Pod in `default` namespace\nkubectl get pods\n# NAME                                             READY   STATUS    RESTARTS   AGE\n# kuberay-apiserver-857869f665-b94px               1/1     Running   0          86m\n# kuberay-operator-7456c6b69b-t6pt7                1/1     Running   0          172m\n</code></pre> <ul> <li>Install the nightly version</li> </ul> <pre><code># Step1: Clone KubeRay repository\n\n# Step2: Move to `helm-chart/kuberay-apiserver`\ncd helm-chart/kuberay-apiserver\n\n# Step3: Install KubeRay APIServer\nhelm install kuberay-apiserver .\n</code></pre> <ul> <li>Install the current (working branch) version</li> </ul> <pre><code># Step1: Clone KubeRay repository\n\n# Step2: Change directory to the api server\ncd apiserver\n\n# Step3: Build docker image, create a local kind cluster and deploy api server (using helm)\nmake docker-image cluster load-image deploy\n</code></pre>"},{"location":"components/apiserver/#list-the-chart","title":"List the chart","text":"<p>To list the deployments:</p> <pre><code>helm ls\n# NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION\n# kuberay-apiserver       default         1               2023-09-25 10:42:34.267328 +0300 EEST   deployed        kuberay-apiserver-1.0.0         \n# kuberay-operator        default         1               2023-09-25 10:41:48.355831 +0300 EEST   deployed        kuberay-operator-1.0.0               \n</code></pre>"},{"location":"components/apiserver/#uninstall-the-chart","title":"Uninstall the Chart","text":"<pre><code># Uninstall the `kuberay-apiserver` release\nhelm uninstall kuberay-apiserver\n\n# The KubeRay APIServer Pod should be removed.\nkubectl get pods\n# No resources found in default namespace.\n</code></pre>"},{"location":"components/apiserver/#usage","title":"Usage","text":"<p>After the deployment we may use the <code>{{baseUrl}}</code> to access the service. See swagger support section to get the complete definitions of APIs.</p> <ul> <li> <p>(default) for nodeport access, use port <code>31888</code> for connection</p> </li> <li> <p>for ingress access, you will need to create your own ingress</p> </li> </ul> <p>The requests parameters detail can be seen in KubeRay swagger, this document only presents basic examples.</p>"},{"location":"components/apiserver/#setup-a-smoke-test","title":"Setup a smoke test","text":"<p>The following steps allow you to validate that the KubeRay API Server components and KubeRay Operator integrate in your environment.</p> <ol> <li> <p>(Optional) You may use your local kind cluster or minikube</p> <pre><code>cat &lt;&lt;EOF | kind create cluster --name ray-test --config -\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  image: kindest/node:v1.23.17@sha256:59c989ff8a517a93127d4a536e7014d28e235fb3529d9fba91b3951d461edfdb\n  kubeadmConfigPatches:\n    - |\n      kind: InitConfiguration\n      nodeRegistration:\n        kubeletExtraArgs:\n          node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 30265\n    hostPort: 8265\n    listenAddress: \"0.0.0.0\"\n    protocol: tcp\n  - containerPort: 30001\n    hostPort: 10001\n    listenAddress: \"0.0.0.0\"\n    protocol: tcp\n  - containerPort: 8000\n    hostPort: 8000\n    listenAddress: \"0.0.0.0\"\n  - containerPort: 31888\n    hostPort: 31888\n    listenAddress: \"0.0.0.0\"\n  - containerPort: 31887 \n    hostPort: 31887 \n    listenAddress: \"0.0.0.0\"\n- role: worker\n  image: kindest/node:v1.23.17@sha256:59c989ff8a517a93127d4a536e7014d28e235fb3529d9fba91b3951d461edfdb\n- role: worker\n  image: kindest/node:v1.23.17@sha256:59c989ff8a517a93127d4a536e7014d28e235fb3529d9fba91b3951d461edfdb\nEOF\n</code></pre> </li> <li> <p>Deploy the KubeRay APIServer within the same cluster of KubeRay operator</p> <pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm -n ray-system install kuberay-apiserver kuberay/kuberay-apiserver -n ray-system --create-namespace\n</code></pre> </li> <li> <p>The APIServer expose service using <code>NodePort</code> by default. You can test access by your host and port, the default port is set to <code>31888</code>. The examples below assume a kind (localhost) deployment. If Kuberay API server is deployed on another type of cluster, you'll need to adjust the hostname to match your environment.</p> <pre><code>curl localhost:31888\n...\n{\n\"code\": 5,\n  \"message\": \"Not Found\"\n}\n</code></pre> </li> <li> <p>You can create <code>RayCluster</code>, <code>RayJobs</code> or <code>RayService</code> by dialing the endpoints.</p> </li> </ol>"},{"location":"components/apiserver/#swagger-support","title":"Swagger Support","text":"<p>Kuberay API server has support for Swagger UI. The swagger page can be reached at:</p> <ul> <li>localhost:31888/swagger-ui for local kind deployments</li> <li>localhost:8888/swagger-ui for instances started with <code>make run</code> (development machine builds)</li> <li><code>&lt;host name&gt;:31888/swagger-ui</code> for nodeport deployments</li> </ul>"},{"location":"components/apiserver/#full-definition-endpoints","title":"Full definition endpoints","text":""},{"location":"components/apiserver/#compute-template","title":"Compute Template","text":"<p>For the purpose to simplify the setting of resources, the Kuberay API server abstracts the resource of the pods template resource to the <code>compute template</code>. You can define the resources in the <code>compute template</code> and then choose the appropriate template for your <code>head</code> and <code>workergroup</code> when you are creating the objects of <code>RayCluster</code>, <code>RayJobs</code> or <code>RayService</code>.</p> <p>The full definition of the compute template resource can be found in config.proto or the Kuberay API server swagger doc.</p>"},{"location":"components/apiserver/#create-compute-templates-in-a-given-namespace","title":"Create compute templates in a given namespace","text":"<pre><code>POST {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/compute_templates\n</code></pre> <p>Examples (please make sure that <code>ray-system</code> namespace exists before running this command):</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'POST' \\\n'http://localhost:31888/apis/v1/namespaces/ray-system/compute_templates' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\n      \"name\": \"default-template\",\n      \"namespace\": \"ray-system\",\n      \"cpu\": 2,\n      \"memory\": 4\n    }'\n</code></pre> <ul> <li>Response</li> </ul> <pre><code>{\n\"name\": \"default-template\",\n\"namespace\": \"ray-system\",\n\"cpu\": 2,\n\"memory\": 4\n}\n</code></pre>"},{"location":"components/apiserver/#list-all-compute-templates-in-a-given-namespace","title":"List all compute templates in a given namespace","text":"<pre><code>GET {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/compute_templates\n</code></pre> <p>Examples:</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'GET' \\\n'http://localhost:31888/apis/v1/namespaces/ray-system/compute_templates' \\\n-H 'accept: application/json'\n</code></pre> <ul> <li>Response</li> </ul> <pre><code>{\n\"computeTemplates\": [\n{\n\"name\": \"default-template\",\n\"namespace\": \"ray-system\",\n\"cpu\": 2,\n\"memory\": 4\n}\n]\n}\n</code></pre>"},{"location":"components/apiserver/#list-all-compute-templates-in-all-namespaces","title":"List all compute templates in all namespaces","text":"<pre><code>GET {{baseUrl}}/apis/v1/compute_templates\n</code></pre> <p>Examples:</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'GET' \\\n'http://localhost:31888/apis/v1/compute_templates' \\\n-H 'accept: application/json'\n</code></pre> <ul> <li> <p>Response</p> <pre><code>{\n\"computeTemplates\": [\n{\n\"name\": \"default-template\",\n\"namespace\": \"ray-system\",\n\"cpu\": 2,\n\"memory\": 4\n}\n]\n}\n</code></pre> </li> </ul>"},{"location":"components/apiserver/#get-compute-template-by-name","title":"Get compute template by name","text":"<pre><code>GET {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/compute_templates/&lt;compute_template_name&gt;\n</code></pre> <p>Examples:</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'GET' \\\n'http://localhost:31888/apis/v1/namespaces/ray-system/compute_templates/default-template' \\\n-H 'accept: application/json'\n</code></pre> <ul> <li>Response:</li> </ul> <pre><code>{\n\"name\": \"default-template\",\n\"namespace\": \"ray-system\",\n\"cpu\": 2,\n\"memory\": 4\n}\n</code></pre>"},{"location":"components/apiserver/#delete-compute-template-by-name","title":"Delete compute template by name","text":"<pre><code>DELETE {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/compute_templates/&lt;compute_template_name&gt;\n</code></pre> <p>Examples:</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'DELETE' \\\n'http://localhost:31888/apis/v1/namespaces/ray-system/compute_templates/default-template' \\\n-H 'accept: application/json'\n</code></pre> <ul> <li>Response</li> </ul> <pre><code>{}\n</code></pre>"},{"location":"components/apiserver/#clusters","title":"Clusters","text":""},{"location":"components/apiserver/#create-cluster-in-a-given-namespace","title":"Create cluster in a given namespace","text":"<pre><code>POST {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/clusters\n</code></pre> <p>Examples: (please make sure that template <code>default-template</code> is created before running this request)</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'POST' \\\n'http://localhost:31888/apis/v1/namespaces/ray-system/clusters' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\n  \"name\": \"test-cluster\",\n  \"namespace\": \"ray-system\",\n  \"user\": \"3cpo\",\n  \"version\": \"2.9.0\",\n  \"environment\": \"DEV\",\n  \"clusterSpec\": {\n    \"headGroupSpec\": {\n      \"computeTemplate\": \"default-template\",\n      \"image\": \"rayproject/ray:2.9.0\",\n      \"serviceType\": \"NodePort\",\n      \"rayStartParams\": {\n        \"dashboard-host\": \"0.0.0.0\",\n        \"metrics-export-port\": \"8080\"\n      },\n      \"volumes\": []\n    },\n    \"workerGroupSpec\": [\n      {\n        \"groupName\": \"small-wg\",\n        \"computeTemplate\": \"default-template\",\n        \"image\": \"rayproject/ray:2.9.0\",\n        \"replicas\": 1,\n        \"minReplicas\": 1,\n        \"maxReplicas\": 1,\n        \"rayStartParams\": {\n          \"node-ip-address\": \"$MY_POD_IP\"\n        }\n      }\n    ]\n  }\n}'\n</code></pre> <ul> <li>Response</li> </ul> <pre><code>{\n\"name\": \"test-cluster\",\n\"namespace\": \"ray-system\",\n\"user\": \"3cpo\",\n\"version\": \"2.9.0\",\n\"clusterSpec\": {\n\"headGroupSpec\": {\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"serviceType\": \"NodePort\",\n\"rayStartParams\": {\n\"dashboard-host\": \"0.0.0.0\",\n\"metrics-export-port\": \"8080\"\n}\n},\n\"workerGroupSpec\": [\n{\n\"groupName\": \"small-wg\",\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"replicas\": 1,\n\"minReplicas\": 1,\n\"maxReplicas\": 1,\n\"rayStartParams\": {\n\"node-ip-address\": \"$MY_POD_IP\"\n}\n}\n]\n},\n\"annotations\": {\n\"ray.io/creation-timestamp\": \"2023-09-25 10:48:35.766443417 +0000 UTC\"\n},\n\"createdAt\": \"2023-09-25T10:48:35Z\",\n\"events\": [\n{\n\"id\": \"test-cluster.178817bd10374138\",\n\"name\": \"test-cluster-test-cluster.178817bd10374138\",\n\"createdAt\": \"2023-09-25T08:42:40Z\",\n\"firstTimestamp\": \"2023-09-25T08:42:40Z\",\n\"lastTimestamp\": \"2023-09-25T08:42:40Z\",\n\"reason\": \"Created\",\n\"message\": \"Created service test-cluster-head-svc\",\n\"type\": \"Normal\",\n\"count\": 1\n},\n{\n\"id\": \"test-cluster.178817bd251e9c7c\",\n\"name\": \"test-cluster-test-cluster.178817bd251e9c7c\",\n\"createdAt\": \"2023-09-25T08:42:40Z\",\n\"firstTimestamp\": \"2023-09-25T08:42:40Z\",\n\"lastTimestamp\": \"2023-09-25T08:42:40Z\",\n\"reason\": \"Created\",\n\"message\": \"Created head pod test-cluster-head-rsbmm\",\n\"type\": \"Normal\",\n\"count\": 1\n},\n{\n\"id\": \"test-cluster.178817bd2b74493f\",\n\"name\": \"test-cluster-test-cluster.178817bd2b74493f\",\n\"createdAt\": \"2023-09-25T08:42:40Z\",\n\"firstTimestamp\": \"2023-09-25T08:42:40Z\",\n\"lastTimestamp\": \"2023-09-25T08:42:41Z\",\n\"reason\": \"Created\",\n\"message\": \"Created worker pod \",\n\"type\": \"Normal\",\n\"count\": 2\n}\n]\n}\n</code></pre>"},{"location":"components/apiserver/#list-all-clusters-in-a-given-namespace","title":"List all clusters in a given namespace","text":"<pre><code>GET {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/clusters\n</code></pre> <p>Examples:</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'GET' \\\n'http://localhost:31888/apis/v1/namespaces/ray-system/clusters' \\\n-H 'accept: application/json'\n</code></pre> <ul> <li>Response</li> </ul> <pre><code>{\n\"clusters\": [\n{\n\"name\": \"test-cluster\",\n\"namespace\": \"ray-system\",\n\"user\": \"3cpo\",\n\"version\": \"2.9.0\",\n\"clusterSpec\": {\n\"headGroupSpec\": {\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"serviceType\": \"NodePort\",\n\"rayStartParams\": {\n\"dashboard-host\": \"0.0.0.0\",\n\"metrics-export-port\": \"8080\"\n}\n},\n\"workerGroupSpec\": [\n{\n\"groupName\": \"small-wg\",\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"replicas\": 1,\n\"minReplicas\": 1,\n\"maxReplicas\": 1,\n\"rayStartParams\": {\n\"node-ip-address\": \"$MY_POD_IP\"\n}\n}\n]\n},\n\"annotations\": {\n\"ray.io/creation-timestamp\": \"2023-09-25 10:48:35.766443417 +0000 UTC\"\n},\n\"createdAt\": \"2023-09-25T10:48:35Z\",\n\"clusterState\": \"ready\",\n\"events\": [\n{\n\"id\": \"test-cluster.178817bd10374138\",\n\"name\": \"test-cluster-test-cluster.178817bd10374138\",\n\"createdAt\": \"2023-09-25T08:42:40Z\",\n\"firstTimestamp\": \"2023-09-25T08:42:40Z\",\n\"lastTimestamp\": \"2023-09-25T08:42:40Z\",\n\"reason\": \"Created\",\n\"message\": \"Created service test-cluster-head-svc\",\n\"type\": \"Normal\",\n\"count\": 1\n},\n{\n\"id\": \"test-cluster.178817bd251e9c7c\",\n\"name\": \"test-cluster-test-cluster.178817bd251e9c7c\",\n\"createdAt\": \"2023-09-25T08:42:40Z\",\n\"firstTimestamp\": \"2023-09-25T08:42:40Z\",\n\"lastTimestamp\": \"2023-09-25T08:42:40Z\",\n\"reason\": \"Created\",\n\"message\": \"Created head pod test-cluster-head-rsbmm\",\n\"type\": \"Normal\",\n\"count\": 1\n},\n{\n\"id\": \"test-cluster.178817bd2b74493f\",\n\"name\": \"test-cluster-test-cluster.178817bd2b74493f\",\n\"createdAt\": \"2023-09-25T08:42:40Z\",\n\"firstTimestamp\": \"2023-09-25T08:42:40Z\",\n\"lastTimestamp\": \"2023-09-25T08:42:41Z\",\n\"reason\": \"Created\",\n\"message\": \"Created worker pod \",\n\"type\": \"Normal\",\n\"count\": 2\n},\n{\n\"id\": \"test-cluster.17881e9c2b82c449\",\n\"name\": \"test-cluster-test-cluster.17881e9c2b82c449\",\n\"createdAt\": \"2023-09-25T10:48:35Z\",\n\"firstTimestamp\": \"2023-09-25T10:48:35Z\",\n\"lastTimestamp\": \"2023-09-25T10:48:35Z\",\n\"reason\": \"Created\",\n\"message\": \"Created service test-cluster-head-svc\",\n\"type\": \"Normal\",\n\"count\": 1\n},\n{\n\"id\": \"test-cluster.17881e9c2e9cd4b8\",\n\"name\": \"test-cluster-test-cluster.17881e9c2e9cd4b8\",\n\"createdAt\": \"2023-09-25T10:48:35Z\",\n\"firstTimestamp\": \"2023-09-25T10:48:35Z\",\n\"lastTimestamp\": \"2023-09-25T10:48:35Z\",\n\"reason\": \"Created\",\n\"message\": \"Created head pod test-cluster-head-nglmx\",\n\"type\": \"Normal\",\n\"count\": 1\n},\n{\n\"id\": \"test-cluster.17881e9c34460442\",\n\"name\": \"test-cluster-test-cluster.17881e9c34460442\",\n\"createdAt\": \"2023-09-25T10:48:35Z\",\n\"firstTimestamp\": \"2023-09-25T10:48:35Z\",\n\"lastTimestamp\": \"2023-09-25T10:48:35Z\",\n\"reason\": \"Created\",\n\"message\": \"Created worker pod \",\n\"type\": \"Normal\",\n\"count\": 1\n}\n],\n\"serviceEndpoint\": {\n\"dashboard\": \"31476\",\n\"head\": \"31850\",\n\"metrics\": \"32189\",\n\"redis\": \"30736\"\n}\n}\n]\n}\n</code></pre>"},{"location":"components/apiserver/#list-all-clusters-in-all-namespaces","title":"List all clusters in all namespaces","text":"<pre><code>GET {{baseUrl}}/apis/v1/clusters\n</code></pre> <p>Examples:</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'GET' \\\n'http://localhost:31888/apis/v1/clusters' \\\n-H 'accept: application/json'\n</code></pre> <ul> <li>Response</li> </ul> <pre><code>{\n\"clusters\": [\n{\n\"name\": \"test-cluster\",\n\"namespace\": \"ray-system\",\n\"user\": \"3cpo\",\n\"version\": \"2.9.0\",\n\"clusterSpec\": {\n\"headGroupSpec\": {\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"serviceType\": \"NodePort\",\n\"rayStartParams\": {\n\"dashboard-host\": \"0.0.0.0\",\n\"metrics-export-port\": \"8080\"\n}\n},\n\"workerGroupSpec\": [\n{\n\"groupName\": \"small-wg\",\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"replicas\": 1,\n\"minReplicas\": 1,\n\"maxReplicas\": 1,\n\"rayStartParams\": {\n\"node-ip-address\": \"$MY_POD_IP\"\n}\n}\n]\n},\n\"annotations\": {\n\"ray.io/creation-timestamp\": \"2023-09-25 10:48:35.766443417 +0000 UTC\"\n},\n\"createdAt\": \"2023-09-25T10:48:35Z\",\n\"clusterState\": \"ready\",\n\"events\": [\n{\n\"id\": \"test-cluster.178817bd10374138\",\n\"name\": \"test-cluster-test-cluster.178817bd10374138\",\n\"createdAt\": \"2023-09-25T08:42:40Z\",\n\"firstTimestamp\": \"2023-09-25T08:42:40Z\",\n\"lastTimestamp\": \"2023-09-25T08:42:40Z\",\n\"reason\": \"Created\",\n\"message\": \"Created service test-cluster-head-svc\",\n\"type\": \"Normal\",\n\"count\": 1\n},\n{\n\"id\": \"test-cluster.178817bd251e9c7c\",\n\"name\": \"test-cluster-test-cluster.178817bd251e9c7c\",\n\"createdAt\": \"2023-09-25T08:42:40Z\",\n\"firstTimestamp\": \"2023-09-25T08:42:40Z\",\n\"lastTimestamp\": \"2023-09-25T08:42:40Z\",\n\"reason\": \"Created\",\n\"message\": \"Created head pod test-cluster-head-rsbmm\",\n\"type\": \"Normal\",\n\"count\": 1\n},\n{\n\"id\": \"test-cluster.178817bd2b74493f\",\n\"name\": \"test-cluster-test-cluster.178817bd2b74493f\",\n\"createdAt\": \"2023-09-25T08:42:40Z\",\n\"firstTimestamp\": \"2023-09-25T08:42:40Z\",\n\"lastTimestamp\": \"2023-09-25T08:42:41Z\",\n\"reason\": \"Created\",\n\"message\": \"Created worker pod \",\n\"type\": \"Normal\",\n\"count\": 2\n},\n{\n\"id\": \"test-cluster.17881e9c2b82c449\",\n\"name\": \"test-cluster-test-cluster.17881e9c2b82c449\",\n\"createdAt\": \"2023-09-25T10:48:35Z\",\n\"firstTimestamp\": \"2023-09-25T10:48:35Z\",\n\"lastTimestamp\": \"2023-09-25T10:48:35Z\",\n\"reason\": \"Created\",\n\"message\": \"Created service test-cluster-head-svc\",\n\"type\": \"Normal\",\n\"count\": 1\n},\n{\n\"id\": \"test-cluster.17881e9c2e9cd4b8\",\n\"name\": \"test-cluster-test-cluster.17881e9c2e9cd4b8\",\n\"createdAt\": \"2023-09-25T10:48:35Z\",\n\"firstTimestamp\": \"2023-09-25T10:48:35Z\",\n\"lastTimestamp\": \"2023-09-25T10:48:35Z\",\n\"reason\": \"Created\",\n\"message\": \"Created head pod test-cluster-head-nglmx\",\n\"type\": \"Normal\",\n\"count\": 1\n},\n{\n\"id\": \"test-cluster.17881e9c34460442\",\n\"name\": \"test-cluster-test-cluster.17881e9c34460442\",\n\"createdAt\": \"2023-09-25T10:48:35Z\",\n\"firstTimestamp\": \"2023-09-25T10:48:35Z\",\n\"lastTimestamp\": \"2023-09-25T10:48:35Z\",\n\"reason\": \"Created\",\n\"message\": \"Created worker pod \",\n\"type\": \"Normal\",\n\"count\": 1\n}\n],\n\"serviceEndpoint\": {\n\"dashboard\": \"31476\",\n\"head\": \"31850\",\n\"metrics\": \"32189\",\n\"redis\": \"30736\"\n}\n}\n]\n}  </code></pre>"},{"location":"components/apiserver/#get-cluster-by-its-name-and-namespace","title":"Get cluster by its name and namespace","text":"<pre><code>GET {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/clusters/&lt;cluster_name&gt;\n</code></pre> <p>Examples:</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'GET' \\\n'http://localhost:31888/apis/v1/namespaces/ray-system/clusters' \\\n-H 'accept: application/json'\n</code></pre> <ul> <li>Response</li> </ul> <pre><code>{\n\"clusters\": [\n{\n\"name\": \"test-cluster\",\n\"namespace\": \"ray-system\",\n\"user\": \"3cpo\",\n\"version\": \"2.9.0\",\n\"clusterSpec\": {\n\"headGroupSpec\": {\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"serviceType\": \"NodePort\",\n\"rayStartParams\": {\n\"dashboard-host\": \"0.0.0.0\",\n\"metrics-export-port\": \"8080\"\n}\n},\n\"workerGroupSpec\": [\n{\n\"groupName\": \"small-wg\",\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"replicas\": 1,\n\"minReplicas\": 1,\n\"maxReplicas\": 1,\n\"rayStartParams\": {\n\"node-ip-address\": \"$MY_POD_IP\"\n}\n}\n]\n},\n\"annotations\": {\n\"ray.io/creation-timestamp\": \"2023-09-25 10:48:35.766443417 +0000 UTC\"\n},\n\"createdAt\": \"2023-09-25T10:48:35Z\",\n\"clusterState\": \"ready\",\n\"events\": [\n{\n\"id\": \"test-cluster.178817bd10374138\",\n\"name\": \"test-cluster-test-cluster.178817bd10374138\",\n\"createdAt\": \"2023-09-25T08:42:40Z\",\n\"firstTimestamp\": \"2023-09-25T08:42:40Z\",\n\"lastTimestamp\": \"2023-09-25T08:42:40Z\",\n\"reason\": \"Created\",\n\"message\": \"Created service test-cluster-head-svc\",\n\"type\": \"Normal\",\n\"count\": 1\n},\n{\n\"id\": \"test-cluster.178817bd251e9c7c\",\n\"name\": \"test-cluster-test-cluster.178817bd251e9c7c\",\n\"createdAt\": \"2023-09-25T08:42:40Z\",\n\"firstTimestamp\": \"2023-09-25T08:42:40Z\",\n\"lastTimestamp\": \"2023-09-25T08:42:40Z\",\n\"reason\": \"Created\",\n\"message\": \"Created head pod test-cluster-head-rsbmm\",\n\"type\": \"Normal\",\n\"count\": 1\n},\n{\n\"id\": \"test-cluster.178817bd2b74493f\",\n\"name\": \"test-cluster-test-cluster.178817bd2b74493f\",\n\"createdAt\": \"2023-09-25T08:42:40Z\",\n\"firstTimestamp\": \"2023-09-25T08:42:40Z\",\n\"lastTimestamp\": \"2023-09-25T08:42:41Z\",\n\"reason\": \"Created\",\n\"message\": \"Created worker pod \",\n\"type\": \"Normal\",\n\"count\": 2\n},\n{\n\"id\": \"test-cluster.17881e9c2b82c449\",\n\"name\": \"test-cluster-test-cluster.17881e9c2b82c449\",\n\"createdAt\": \"2023-09-25T10:48:35Z\",\n\"firstTimestamp\": \"2023-09-25T10:48:35Z\",\n\"lastTimestamp\": \"2023-09-25T10:48:35Z\",\n\"reason\": \"Created\",\n\"message\": \"Created service test-cluster-head-svc\",\n\"type\": \"Normal\",\n\"count\": 1\n},\n{\n\"id\": \"test-cluster.17881e9c2e9cd4b8\",\n\"name\": \"test-cluster-test-cluster.17881e9c2e9cd4b8\",\n\"createdAt\": \"2023-09-25T10:48:35Z\",\n\"firstTimestamp\": \"2023-09-25T10:48:35Z\",\n\"lastTimestamp\": \"2023-09-25T10:48:35Z\",\n\"reason\": \"Created\",\n\"message\": \"Created head pod test-cluster-head-nglmx\",\n\"type\": \"Normal\",\n\"count\": 1\n},\n{\n\"id\": \"test-cluster.17881e9c34460442\",\n\"name\": \"test-cluster-test-cluster.17881e9c34460442\",\n\"createdAt\": \"2023-09-25T10:48:35Z\",\n\"firstTimestamp\": \"2023-09-25T10:48:35Z\",\n\"lastTimestamp\": \"2023-09-25T10:48:35Z\",\n\"reason\": \"Created\",\n\"message\": \"Created worker pod \",\n\"type\": \"Normal\",\n\"count\": 1\n}\n],\n\"serviceEndpoint\": {\n\"dashboard\": \"31476\",\n\"head\": \"31850\",\n\"metrics\": \"32189\",\n\"redis\": \"30736\"\n}\n}\n]\n}\n</code></pre>"},{"location":"components/apiserver/#delete-cluster-by-its-name-and-namespace","title":"Delete cluster by its name and namespace","text":"<pre><code>DELETE {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/clusters/&lt;cluster_name&gt;\n</code></pre> <p>Examples:</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'DELETE' \\\n'http://localhost:31888/apis/v1/namespaces/ray-system/clusters/test-cluster' \\\n-H 'accept: application/json'\n</code></pre> <ul> <li>Response:</li> </ul> <pre><code>{}\n</code></pre>"},{"location":"components/apiserver/#rayjob","title":"RayJob","text":""},{"location":"components/apiserver/#create-ray-job-in-a-given-namespace","title":"Create ray job in a given namespace","text":"<pre><code>POST {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/jobs\n</code></pre> <p>Examples:</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'POST' \\\n'http://localhost:31888/apis/v1/namespaces/ray-system/jobs' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"name\": \"rayjob-test\",\n    \"namespace\": \"ray-system\",\n    \"user\": \"3cp0\",\n    \"version\": \"2.9.0\",\n    \"entrypoint\": \"python -V\",\n    \"clusterSpec\": {\n      \"headGroupSpec\": {\n        \"computeTemplate\": \"default-template\",\n        \"image\": \"rayproject/ray:2.9.0\",\n        \"serviceType\": \"NodePort\",\n        \"rayStartParams\": {\n          \"dashboard-host\": \"0.0.0.0\"\n        }\n      },\n      \"workerGroupSpec\": [\n        {\n          \"groupName\": \"small-wg\",\n          \"computeTemplate\": \"default-template\",\n          \"image\": \"rayproject/ray:2.9.0\",\n          \"replicas\": 1,\n          \"minReplicas\": 0,\n          \"maxReplicas\": 1,\n          \"rayStartParams\": {\n            \"metrics-export-port\": \"8080\"\n          }\n        }\n      ]\n    }\n  }'\n</code></pre> <ul> <li>Response</li> </ul> <pre><code>{\n\"name\": \"rayjob-test\",\n\"namespace\": \"ray-system\",\n\"user\": \"3cp0\",\n\"entrypoint\": \"python -V\",\n\"clusterSpec\": {\n\"headGroupSpec\": {\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"serviceType\": \"NodePort\",\n\"rayStartParams\": {\n\"dashboard-host\": \"0.0.0.0\"\n}\n},\n\"workerGroupSpec\": [\n{\n\"groupName\": \"small-wg\",\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"replicas\": 1,\n\"minReplicas\": 1,\n\"maxReplicas\": 1,\n\"rayStartParams\": {\n\"metrics-export-port\": \"8080\"\n}\n}\n]\n},\n\"createdAt\": \"2023-09-25T11:36:02Z\"\n}\n</code></pre> <p>The above example creates a new Ray cluster, executes a job on it and optionally deletes a cluster. As an alternative, the same command allows creating a new job on the existing cluster by referencing it in the payload.</p> <p>Examples:</p> <p>Start from creating Ray cluster (We assume here that the template and configmap are already created).</p> <ul> <li>Request</li> </ul> <pre><code>curl -X POST 'localhost:31888/apis/v1/namespaces/default/clusters' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"name\": \"job-test\",\n  \"namespace\": \"default\",\n  \"user\": \"boris\",\n  \"version\": \"2.9.0\",\n  \"environment\": \"DEV\",\n  \"clusterSpec\": {\n    \"headGroupSpec\": {\n      \"computeTemplate\": \"default-template\",\n      \"image\": \"rayproject/ray:2.9.0-py310\",\n      \"serviceType\": \"NodePort\",\n      \"rayStartParams\": {\n         \"dashboard-host\": \"0.0.0.0\",\n         \"metrics-export-port\": \"8080\"\n       },\n       \"volumes\": [\n         {\n           \"name\": \"code-sample\",\n           \"mountPath\": \"/home/ray/samples\",\n           \"volumeType\": \"CONFIGMAP\",\n           \"source\": \"ray-job-code-sample\",\n           \"items\": {\"sample_code.py\" : \"sample_code.py\"}\n         }\n       ]\n    },\n    \"workerGroupSpec\": [\n      {\n        \"groupName\": \"small-wg\",\n        \"computeTemplate\": \"default-template\",\n        \"image\": \"rayproject/ray:2.9.0-py310\",\n        \"replicas\": 1,\n        \"minReplicas\": 0,\n        \"maxReplicas\": 5,\n        \"rayStartParams\": {\n           \"node-ip-address\": \"$MY_POD_IP\"\n         },\n        \"volumes\": [\n          {\n            \"name\": \"code-sample\",\n            \"mountPath\": \"/home/ray/samples\",\n            \"volumeType\": \"CONFIGMAP\",\n            \"source\": \"ray-job-code-sample\",\n            \"items\": {\"sample_code.py\" : \"sample_code.py\"}\n          }\n        ]\n      }\n    ]\n  }\n}'\n</code></pre> <ul> <li>Response</li> </ul> <pre><code>{\n\"name\":\"job-test\",\n\"namespace\":\"default\",\n\"user\":\"boris\",\n\"version\":\"2.9.0\",\n\"clusterSpec\":{\n\"headGroupSpec\":{\n\"computeTemplate\":\"default-template\",\n\"image\":\"rayproject/ray:2.9.0-py310\",\n\"serviceType\":\"NodePort\",\n\"rayStartParams\":{\n\"dashboard-host\":\"0.0.0.0\",\n\"metrics-export-port\":\"8080\"\n},\n\"volumes\":[\n{\n\"mountPath\":\"/home/ray/samples\",\n\"volumeType\":3,\n\"name\":\"code-sample\",\n\"source\":\"ray-job-code-sample\",\n\"items\":{\n\"sample_code.py\":\"sample_code.py\"\n}               }\n],\n\"environment\":{\n\n}\n},\n\"workerGroupSpec\":[\n{\n\"groupName\":\"small-wg\",\n\"computeTemplate\":\"default-template\",\n\"image\":\"rayproject/ray:2.9.0-py310\",\n\"replicas\":1,\n\"minReplicas\":5,\n\"maxReplicas\":1,\n\"rayStartParams\":{\n\"node-ip-address\":\"$MY_POD_IP\"\n},\n\"volumes\":[\n{\n\"mountPath\":\"/home/ray/samples\",\n\"volumeType\":3,\n\"name\":\"code-sample\",\n\"source\":\"ray-job-code-sample\",\n\"items\":{\n\"sample_code.py\":\"sample_code.py\"\n}\n}\n],\n\"environment\":{\n\n}\n}\n]\n},\n\"annotations\":{\n\"ray.io/creation-timestamp\":\"2023-10-18 08:47:48.058576 +0000 UTC\"\n},\n\"createdAt\":\"2023-10-18T08:47:48Z\"\n}\n</code></pre> <p>Once the cluster is created, we can create a job to run on it.</p> <ul> <li>Request</li> </ul> <pre><code>curl -X POST 'localhost:31888/apis/v1/namespaces/default/jobs' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"name\": \"job-test\",\n  \"namespace\": \"default\",\n  \"user\": \"boris\",\n  \"version\": \"2.9.0\",  \n  \"entrypoint\": \"python /home/ray/samples/sample_code.py\",\n  \"runtimeEnv\": \"pip:\\n  - requests==2.26.0\\n  - pendulum==2.1.2\\nenv_vars:\\n  counter_name: test_counter\\n\",\n  \"jobSubmitter\": {\n    \"image\": \"rayproject/ray:2.9.0-py310\",\n    \"cpu\": \"400m\",\n    \"memory\": \"150Mi\" \n  },\n  \"clusterSelector\": {\n    \"ray.io/cluster\": \"job-test\"\n  }\n}'\n</code></pre> <ul> <li>Response</li> </ul> <pre><code>{\n\"name\":\"job-test\",\n\"namespace\":\"default\",\n\"user\":\"boris\",\n\"entrypoint\":\"python /home/ray/samples/sample_code.py\",\n\"runtimeEnv\":\"pip:\\n  - requests==2.26.0\\n  - pendulum==2.1.2\\nenv_vars:\\n  counter_name: test_counter\\n\",\n\"clusterSelector\":{\n\"ray.io/cluster\":\"job-test\"\n},\n\"createdAt\":\"2023-10-24T11:37:29Z\"\n}\n</code></pre> <p>You should also see job submitter job completed, something like:</p> <pre><code>job-test-2hhmf                   0/1     Completed   0          15s\n</code></pre> <p>To see job execution results run:</p> <pre><code>kubectl logs job-test-2hhmf </code></pre> <p>And you should get something similar to:</p> <pre><code>2023-10-18 03:19:51,524 INFO cli.py:36 -- Job submission server address: http://job-test-head-svc.default.svc.cluster.local:8265\n2023-10-18 03:19:52,197 SUCC cli.py:60 -- -------------------------------------------\n2023-10-18 03:19:52,197 SUCC cli.py:61 -- Job 'job-test-bbfqs' submitted successfully\n2023-10-18 03:19:52,197 SUCC cli.py:62 -- -------------------------------------------\n2023-10-18 03:19:52,197 INFO cli.py:274 -- Next steps\n2023-10-18 03:19:52,197 INFO cli.py:275 -- Query the logs of the job:\n2023-10-18 03:19:52,198 INFO cli.py:277 -- ray job logs job-test-bbfqs\n2023-10-18 03:19:52,198 INFO cli.py:279 -- Query the status of the job:\n2023-10-18 03:19:52,198 INFO cli.py:281 -- ray job status job-test-bbfqs\n2023-10-18 03:19:52,198 INFO cli.py:283 -- Request the job to be stopped:\n2023-10-18 03:19:52,198 INFO cli.py:285 -- ray job stop job-test-bbfqs\n2023-10-18 03:19:52,203 INFO cli.py:292 -- Tailing logs until the job exits (disable with --no-wait):\n2023-10-18 03:20:00,014 INFO worker.py:1329 -- Using address 10.244.0.10:6379 set in the environment variable RAY_ADDRESS\n2023-10-18 03:20:00,014 INFO worker.py:1458 -- Connecting to existing Ray cluster at address: 10.244.0.10:6379...\n2023-10-18 03:20:00,032 INFO worker.py:1633 -- Connected to Ray cluster. View the dashboard at 10.244.0.10:8265 \ntest_counter got 1\ntest_counter got 2\ntest_counter got 3\ntest_counter got 4\ntest_counter got 5\n2023-10-18 03:20:03,304 SUCC cli.py:60 -- ------------------------------\n2023-10-18 03:20:03,304 SUCC cli.py:61 -- Job 'job-test-bbfqs' succeeded\n2023-10-18 03:20:03,304 SUCC cli.py:62 -- ------------------------------\n</code></pre> <p>Additionally here, we can specify configuration for the job submitter, allowing to specify image, memory and cpu limits for it.</p> <p>Make sure that you delete previous job before running this one:</p> <pre><code>kubectl delete rayjob job-test\n</code></pre> <ul> <li>Request</li> </ul> <pre><code>curl -X POST 'localhost:31888/apis/v1/namespaces/default/jobs' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"name\": \"job-test\",\n  \"namespace\": \"default\",\n  \"user\": \"boris\",\n  \"version\": \"2.9.0\",\n  \"entrypoint\": \"python /home/ray/samples/sample_code.py\",\n   \"runtimeEnv\": \"pip:\\n  - requests==2.26.0\\n  - pendulum==2.1.2\\nenv_vars:\\n  counter_name: test_counter\\n\",\n  \"clusterSelector\": {\n    \"ray.io/cluster\": \"job-test\"\n  },\n  \"jobSubmitter\": {\n    \"image\": \"rayproject/ray:2.9.0-py310\"\n  }\n}'  </code></pre> <ul> <li>Response</li> </ul> <pre><code>{\n\"name\":\"job-test\",\n\"namespace\":\"default\",\n\"user\":\"boris\",\n\"entrypoint\":\"python /home/ray/samples/sample_code.py\",\n\"runtimeEnv\":\"pip:\\n  - requests==2.26.0\\n  - pendulum==2.1.2\\nenv_vars:\\n  counter_name: test_counter\\n\",\n\"clusterSelector\":{\n\"ray.io/cluster\":\"job-test\"\n},\n\"jobSubmitter\":{\n\"image\":\"rayproject/ray:2.9.0-py310\"\n},\n\"createdAt\":\"2023-10-24T11:48:19Z\"\n}\n</code></pre> <p>You should beble to see job execution results similar to above</p>"},{"location":"components/apiserver/#list-all-jobs-in-a-given-namespace","title":"List all jobs in a given namespace","text":"<pre><code>GET {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/jobs\n</code></pre> <p>Examples:</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'GET' \\\n'http://localhost:31888/apis/v1/namespaces/ray-system/jobs' \\\n-H 'accept: application/json'\n</code></pre> <ul> <li>Response</li> </ul> <pre><code>{\n\"jobs\": [\n{\n\"name\": \"rayjob-test\",\n\"namespace\": \"ray-system\",\n\"user\": \"3cp0\",\n\"entrypoint\": \"python -V\",\n\"jobId\": \"rayjob-test-drhlq\",\n\"clusterSpec\": {\n\"headGroupSpec\": {\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"serviceType\": \"NodePort\",\n\"rayStartParams\": {\n\"dashboard-host\": \"0.0.0.0\"\n}\n},\n\"workerGroupSpec\": [\n{\n\"groupName\": \"small-wg\",\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"replicas\": 1,\n\"minReplicas\": 1,\n\"maxReplicas\": 1,\n\"rayStartParams\": {\n\"metrics-export-port\": \"8080\"\n}\n}\n]\n},\n\"createdAt\": \"2023-09-25T11:36:02Z\",\n\"jobStatus\": \"SUCCEEDED\",\n\"jobDeploymentStatus\": \"Running\",\n\"message\": \"Job finished successfully.\"\n}\n]\n}\n</code></pre>"},{"location":"components/apiserver/#list-all-jobs-in-all-namespaces","title":"List all jobs in all namespaces","text":"<pre><code>GET {{baseUrl}}/apis/v1/jobs\n</code></pre> <p>Examples:</p> <ul> <li>Request:</li> </ul> <pre><code>curl --silent -X 'GET' \\\n'http://localhost:31888/apis/v1/jobs' \\\n-H 'accept: application/json' </code></pre> <ul> <li>Response</li> </ul> <pre><code>{\n\"jobs\": [\n{\n\"name\": \"rayjob-test\",\n\"namespace\": \"ray-system\",\n\"user\": \"3cp0\",\n\"entrypoint\": \"python -V\",\n\"jobId\": \"rayjob-test-drhlq\",\n\"clusterSpec\": {\n\"headGroupSpec\": {\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"serviceType\": \"NodePort\",\n\"rayStartParams\": {\n\"dashboard-host\": \"0.0.0.0\"\n}\n},\n\"workerGroupSpec\": [\n{\n\"groupName\": \"small-wg\",\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"replicas\": 1,\n\"minReplicas\": 1,\n\"maxReplicas\": 1,\n\"rayStartParams\": {\n\"metrics-export-port\": \"8080\"\n}\n}\n]\n},\n\"createdAt\": \"2023-09-25T11:36:02Z\",\n\"jobStatus\": \"SUCCEEDED\",\n\"jobDeploymentStatus\": \"Running\",\n\"message\": \"Job finished successfully.\"\n}\n]\n}\n</code></pre>"},{"location":"components/apiserver/#get-job-by-its-name-and-namespace","title":"Get job by its name and namespace","text":"<pre><code>GET {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/jobs/&lt;job_name&gt;\n</code></pre> <p>Examples:</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'GET' \\\n'http://localhost:31888/apis/v1/namespaces/ray-system/jobs/rayjob-test' \\\n-H 'accept: application/json'\n</code></pre> <ul> <li>Response:</li> </ul> <pre><code>{\n\"name\": \"rayjob-test\",\n\"namespace\": \"ray-system\",\n\"user\": \"3cp0\",\n\"entrypoint\": \"python -V\",\n\"jobId\": \"rayjob-test-drhlq\",\n\"clusterSpec\": {\n\"headGroupSpec\": {\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"serviceType\": \"NodePort\",\n\"rayStartParams\": {\n\"dashboard-host\": \"0.0.0.0\"\n}\n},\n\"workerGroupSpec\": [\n{\n\"groupName\": \"small-wg\",\n\"computeTemplate\": \"default-template\",\n\"image\": \"rayproject/ray:2.9.0\",\n\"replicas\": 1,\n\"minReplicas\": 1,\n\"maxReplicas\": 1,\n\"rayStartParams\": {\n\"metrics-export-port\": \"8080\"\n}\n}\n]\n},\n\"createdAt\": \"2023-09-25T11:36:02Z\",\n\"jobStatus\": \"SUCCEEDED\",\n\"jobDeploymentStatus\": \"Running\",\n\"message\": \"Job finished successfully.\"\n}\n</code></pre>"},{"location":"components/apiserver/#delete-job-by-its-name-and-namespace","title":"Delete job by its name and namespace","text":"<pre><code>DELETE {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/jobs/&lt;job_name&gt;\n</code></pre> <p>Examples:</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'DELETE' \\\n'http://localhost:31888/apis/v1/namespaces/ray-system/jobs/rayjob-test' \\\n-H 'accept: application/json'\n</code></pre> <ul> <li>Response</li> </ul> <pre><code>{}\n</code></pre>"},{"location":"components/apiserver/#rayservice","title":"RayService","text":""},{"location":"components/apiserver/#create-ray-service-in-a-given-namespace","title":"Create ray service in a given namespace","text":"<pre><code>POST {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/services\n</code></pre> <p>Examples:</p> <ul> <li>Request</li> </ul> <pre><code> curl -X 'POST' 'http://localhost:31888/apis/v1/namespaces/default/services' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '\n  {\n    \"name\": \"test-v2\",\n    \"namespace\": \"default\",\n    \"user\": \"user\",\n    \"version\": \"2.9.0\",\n    \"serveConfigV2\": \"applications:\\n  - name: fruit_app\\n    import_path: fruit.deployment_graph\\n    route_prefix: /fruit\\n    runtime_env:\\n      working_dir: \\\"https://github.com/ray-project/test_dag/archive/78b4a5da38796123d9f9ffff59bab2792a043e95.zip\\\"\\n    deployments:\\n      - name: MangoStand\\n        num_replicas: 2\\n        max_replicas_per_node: 1\\n        user_config:\\n          price: 3\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: OrangeStand\\n        num_replicas: 1\\n        user_config:\\n          price: 2\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: PearStand\\n        num_replicas: 1\\n        user_config:\\n          price: 1\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: FruitMarket\\n        num_replicas: 1\\n        ray_actor_options:\\n          num_cpus: 0.1\\n  - name: math_app\\n    import_path: conditional_dag.serve_dag\\n    route_prefix: /calc\\n    runtime_env:\\n      working_dir: \\\"https://github.com/ray-project/test_dag/archive/78b4a5da38796123d9f9ffff59bab2792a043e95.zip\\\"\\n    deployments:\\n      - name: Adder\\n        num_replicas: 1\\n        user_config:\\n          increment: 3\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: Multiplier\\n        num_replicas: 1\\n        user_config:\\n          factor: 5\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: Router\\n        num_replicas: 1\\n\",\n    \"clusterSpec\": {\n      \"headGroupSpec\": {\n        \"computeTemplate\": \"default-template\",\n        \"image\": \"rayproject/ray:2.9.0-py310\",\n        \"serviceType\": \"NodePort\",\n        \"rayStartParams\": {\n          \"dashboard-host\": \"0.0.0.0\",\n          \"metrics-export-port\": \"8080\"\n        },\n        \"volumes\": []\n      },\n      \"workerGroupSpec\": [\n        {\n          \"groupName\": \"small-wg\",\n          \"computeTemplate\": \"default-template\",\n          \"image\": \"rayproject/ray:2.9.0-py310\",\n          \"replicas\": 1,\n          \"minReplicas\": 0,\n          \"maxReplicas\": 5,\n          \"rayStartParams\": {\n            \"node-ip-address\": \"$MY_POD_IP\"\n          }\n        }\n      ]\n    }\n  }'\n</code></pre> <ul> <li>Response</li> </ul> <pre><code>{\n\"name\":\"test-v2\",\n\"namespace\":\"default\",\n\"user\":\"user\",\n\"serveConfigV2\":\"applications:\\n  - name: fruit_app\\n    import_path: fruit.deployment_graph\\n    route_prefix: /fruit\\n    runtime_env:\\n      working_dir: \\\"https://github.com/ray-project/test_dag/archive/78b4a5da38796123d9f9ffff59bab2792a043e95.zip\\\"\\n    deployments:\\n      - name: MangoStand\\n        num_replicas: 2\\n        max_replicas_per_node: 1\\n        user_config:\\n          price: 3\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: OrangeStand\\n        num_replicas: 1\\n        user_config:\\n          price: 2\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: PearStand\\n        num_replicas: 1\\n        user_config:\\n          price: 1\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: FruitMarket\\n        num_replicas: 1\\n        ray_actor_options:\\n          num_cpus: 0.1\\n  - name: math_app\\n    import_path: conditional_dag.serve_dag\\n    route_prefix: /calc\\n    runtime_env:\\n      working_dir: \\\"https://github.com/ray-project/test_dag/archive/78b4a5da38796123d9f9ffff59bab2792a043e95.zip\\\"\\n    deployments:\\n      - name: Adder\\n        num_replicas: 1\\n        user_config:\\n          increment: 3\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: Multiplier\\n        num_replicas: 1\\n        user_config:\\n          factor: 5\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: Router\\n        num_replicas: 1\\n\",\n\"clusterSpec\":{\n\"headGroupSpec\":{\n\"computeTemplate\":\"default-template\",\n\"image\":\"rayproject/ray:2.9.0-py310\",\n\"serviceType\":\"NodePort\",\n\"rayStartParams\":{\n\"dashboard-host\":\"0.0.0.0\",\n\"metrics-export-port\":\"8080\"\n},\n\"environment\":{\n\n}\n},\n\"workerGroupSpec\":[\n{\n\"groupName\":\"small-wg\",\n\"computeTemplate\":\"default-template\",\n\"image\":\"rayproject/ray:2.9.0-py310\",\n\"replicas\":1,\n\"minReplicas\":1,\n\"maxReplicas\":5,\n\"rayStartParams\":{\n\"node-ip-address\":\"$MY_POD_IP\"\n},\n\"environment\":{\n\n}\n}\n]\n},\n\"rayServiceStatus\":{\n\"rayServiceEvents\":[\n{\n\"id\":\"test-v2.17ab175ec787d26e\",\n\"name\":\"test-v2-test-v2.17ab175ec787d26e\",\n\"createdAt\":\"2024-01-17T09:09:39Z\",\n\"firstTimestamp\":\"2024-01-17T09:09:39Z\",\n\"lastTimestamp\":\"2024-01-17T09:10:04Z\",\n\"reason\":\"ServiceNotReady\",\n\"message\":\"The service is not ready yet. Controller will perform a round of actions in 2s.\",\n\"type\":\"Normal\",\n\"count\":13\n},\n{\n\"id\":\"test-v2.17ab176292337141\",\n\"name\":\"test-v2-test-v2.17ab176292337141\",\n\"createdAt\":\"2024-01-17T09:09:56Z\",\n\"firstTimestamp\":\"2024-01-17T09:09:56Z\",\n\"lastTimestamp\":\"2024-01-17T09:09:56Z\",\n\"reason\":\"SubmittedServeDeployment\",\n\"message\":\"Controller sent API request to update Serve deployments on cluster test-v2-raycluster-lmk5c\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2.17ab1764fe827fe3\",\n\"name\":\"test-v2-test-v2.17ab1764fe827fe3\",\n\"createdAt\":\"2024-01-17T09:10:06Z\",\n\"firstTimestamp\":\"2024-01-17T09:10:06Z\",\n\"lastTimestamp\":\"2024-01-17T09:14:41Z\",\n\"reason\":\"Running\",\n\"message\":\"The Serve applicaton is now running and healthy.\",\n\"type\":\"Normal\",\n\"count\":139\n},\n{\n\"id\":\"test-v2.17ab1815ce6a98da\",\n\"name\":\"test-v2-test-v2.17ab1815ce6a98da\",\n\"createdAt\":\"2024-01-17T09:22:45Z\",\n\"firstTimestamp\":\"2024-01-17T09:22:45Z\",\n\"lastTimestamp\":\"2024-01-17T09:23:16Z\",\n\"reason\":\"ServiceNotReady\",\n\"message\":\"The service is not ready yet. Controller will perform a round of actions in 2s.\",\n\"type\":\"Normal\",\n\"count\":16\n},\n{\n\"id\":\"test-v2.17ab181a8576812d\",\n\"name\":\"test-v2-test-v2.17ab181a8576812d\",\n\"createdAt\":\"2024-01-17T09:23:06Z\",\n\"firstTimestamp\":\"2024-01-17T09:23:06Z\",\n\"lastTimestamp\":\"2024-01-17T09:23:06Z\",\n\"reason\":\"SubmittedServeDeployment\",\n\"message\":\"Controller sent API request to update Serve deployments on cluster test-v2-raycluster-b85bj\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2.17ab181d64327965\",\n\"name\":\"test-v2-test-v2.17ab181d64327965\",\n\"createdAt\":\"2024-01-17T09:23:18Z\",\n\"firstTimestamp\":\"2024-01-17T09:23:18Z\",\n\"lastTimestamp\":\"2024-01-17T09:23:26Z\",\n\"reason\":\"Running\",\n\"message\":\"The Serve applicaton is now running and healthy.\",\n\"type\":\"Normal\",\n\"count\":8\n},\n{\n\"id\":\"test-v2.17ab1857fcf7e3a4\",\n\"name\":\"test-v2-test-v2.17ab1857fcf7e3a4\",\n\"createdAt\":\"2024-01-17T09:27:30Z\",\n\"firstTimestamp\":\"2024-01-17T09:27:30Z\",\n\"lastTimestamp\":\"2024-01-17T09:27:58Z\",\n\"reason\":\"ServiceNotReady\",\n\"message\":\"The service is not ready yet. Controller will perform a round of actions in 2s.\",\n\"type\":\"Normal\",\n\"count\":15\n},\n{\n\"id\":\"test-v2.17ab185bc523559a\",\n\"name\":\"test-v2-test-v2.17ab185bc523559a\",\n\"createdAt\":\"2024-01-17T09:27:46Z\",\n\"firstTimestamp\":\"2024-01-17T09:27:46Z\",\n\"lastTimestamp\":\"2024-01-17T09:27:46Z\",\n\"reason\":\"SubmittedServeDeployment\",\n\"message\":\"Controller sent API request to update Serve deployments on cluster test-v2-raycluster-lbl9x\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2.17ab185f245800ff\",\n\"name\":\"test-v2-test-v2.17ab185f245800ff\",\n\"createdAt\":\"2024-01-17T09:28:00Z\",\n\"firstTimestamp\":\"2024-01-17T09:28:00Z\",\n\"lastTimestamp\":\"2024-01-17T09:28:11Z\",\n\"reason\":\"Running\",\n\"message\":\"The Serve applicaton is now running and healthy.\",\n\"type\":\"Normal\",\n\"count\":9\n}\n]\n},\n\"createdAt\":\"2024-01-17T09:31:34Z\",\n\"deleteAt\":\"1969-12-31T23:59:59Z\"\n}  </code></pre>"},{"location":"components/apiserver/#list-all-services-in-a-given-namespace","title":"List all services in a given namespace","text":"<pre><code>GET {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/services\n</code></pre> <p>Examples</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'GET' \\\n'http://localhost:31888/apis/v1/namespaces/default/services' \\\n-H 'accept: application/json'\n</code></pre> <ul> <li>Response:</li> </ul> <pre><code>  {\n\"services\":[\n{\n\"name\":\"test-v2\",\n\"namespace\":\"default\",\n\"user\":\"user\",\n\"serveConfigV2\":\"applications:\\n  - name: fruit_app\\n    import_path: fruit.deployment_graph\\n    route_prefix: /fruit\\n    runtime_env:\\n      working_dir: \\\"https://github.com/ray-project/test_dag/archive/78b4a5da38796123d9f9ffff59bab2792a043e95.zip\\\"\\n    deployments:\\n      - name: MangoStand\\n        num_replicas: 2\\n        max_replicas_per_node: 1\\n        user_config:\\n          price: 3\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: OrangeStand\\n        num_replicas: 1\\n        user_config:\\n          price: 2\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: PearStand\\n        num_replicas: 1\\n        user_config:\\n          price: 1\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: FruitMarket\\n        num_replicas: 1\\n        ray_actor_options:\\n          num_cpus: 0.1\\n  - name: math_app\\n    import_path: conditional_dag.serve_dag\\n    route_prefix: /calc\\n    runtime_env:\\n      working_dir: \\\"https://github.com/ray-project/test_dag/archive/78b4a5da38796123d9f9ffff59bab2792a043e95.zip\\\"\\n    deployments:\\n      - name: Adder\\n        num_replicas: 1\\n        user_config:\\n          increment: 3\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: Multiplier\\n        num_replicas: 1\\n        user_config:\\n          factor: 5\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: Router\\n        num_replicas: 1\\n\",\n\"clusterSpec\":{\n\"headGroupSpec\":{\n\"computeTemplate\":\"default-template\",\n\"image\":\"rayproject/ray:2.9.0-py310\",\n\"serviceType\":\"NodePort\",\n\"rayStartParams\":{\n\"dashboard-host\":\"0.0.0.0\",\n\"metrics-export-port\":\"8080\"\n},\n\"environment\":{\n\n}\n},\n\"workerGroupSpec\":[\n{\n\"groupName\":\"small-wg\",\n\"computeTemplate\":\"default-template\",\n\"image\":\"rayproject/ray:2.9.0-py310\",\n\"replicas\":1,\n\"minReplicas\":1,\n\"maxReplicas\":5,\n\"rayStartParams\":{\n\"node-ip-address\":\"$MY_POD_IP\"\n},\n\"environment\":{\n\n}\n}\n]\n},\n\"rayServiceStatus\":{\n\"rayServiceEvents\":[\n{\n\"id\":\"test-v2.17ab175ec787d26e\",\n\"name\":\"test-v2-test-v2.17ab175ec787d26e\",\n\"createdAt\":\"2024-01-17T09:09:39Z\",\n\"firstTimestamp\":\"2024-01-17T09:09:39Z\",\n\"lastTimestamp\":\"2024-01-17T09:10:04Z\",\n\"reason\":\"ServiceNotReady\",\n\"message\":\"The service is not ready yet. Controller will perform a round of actions in 2s.\",\n\"type\":\"Normal\",\n\"count\":13\n},\n{\n\"id\":\"test-v2.17ab176292337141\",\n\"name\":\"test-v2-test-v2.17ab176292337141\",\n\"createdAt\":\"2024-01-17T09:09:56Z\",\n\"firstTimestamp\":\"2024-01-17T09:09:56Z\",\n\"lastTimestamp\":\"2024-01-17T09:09:56Z\",\n\"reason\":\"SubmittedServeDeployment\",\n\"message\":\"Controller sent API request to update Serve deployments on cluster test-v2-raycluster-lmk5c\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2.17ab1764fe827fe3\",\n\"name\":\"test-v2-test-v2.17ab1764fe827fe3\",\n\"createdAt\":\"2024-01-17T09:10:06Z\",\n\"firstTimestamp\":\"2024-01-17T09:10:06Z\",\n\"lastTimestamp\":\"2024-01-17T09:14:41Z\",\n\"reason\":\"Running\",\n\"message\":\"The Serve applicaton is now running and healthy.\",\n\"type\":\"Normal\",\n\"count\":139\n},\n{\n\"id\":\"test-v2.17ab1815ce6a98da\",\n\"name\":\"test-v2-test-v2.17ab1815ce6a98da\",\n\"createdAt\":\"2024-01-17T09:22:45Z\",\n\"firstTimestamp\":\"2024-01-17T09:22:45Z\",\n\"lastTimestamp\":\"2024-01-17T09:23:16Z\",\n\"reason\":\"ServiceNotReady\",\n\"message\":\"The service is not ready yet. Controller will perform a round of actions in 2s.\",\n\"type\":\"Normal\",\n\"count\":16\n},\n{\n\"id\":\"test-v2.17ab181a8576812d\",\n\"name\":\"test-v2-test-v2.17ab181a8576812d\",\n\"createdAt\":\"2024-01-17T09:23:06Z\",\n\"firstTimestamp\":\"2024-01-17T09:23:06Z\",\n\"lastTimestamp\":\"2024-01-17T09:23:06Z\",\n\"reason\":\"SubmittedServeDeployment\",\n\"message\":\"Controller sent API request to update Serve deployments on cluster test-v2-raycluster-b85bj\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2.17ab181d64327965\",\n\"name\":\"test-v2-test-v2.17ab181d64327965\",\n\"createdAt\":\"2024-01-17T09:23:18Z\",\n\"firstTimestamp\":\"2024-01-17T09:23:18Z\",\n\"lastTimestamp\":\"2024-01-17T09:23:26Z\",\n\"reason\":\"Running\",\n\"message\":\"The Serve applicaton is now running and healthy.\",\n\"type\":\"Normal\",\n\"count\":8\n},\n{\n\"id\":\"test-v2.17ab1857fcf7e3a4\",\n\"name\":\"test-v2-test-v2.17ab1857fcf7e3a4\",\n\"createdAt\":\"2024-01-17T09:27:30Z\",\n\"firstTimestamp\":\"2024-01-17T09:27:30Z\",\n\"lastTimestamp\":\"2024-01-17T09:27:58Z\",\n\"reason\":\"ServiceNotReady\",\n\"message\":\"The service is not ready yet. Controller will perform a round of actions in 2s.\",\n\"type\":\"Normal\",\n\"count\":15\n},\n{\n\"id\":\"test-v2.17ab185bc523559a\",\n\"name\":\"test-v2-test-v2.17ab185bc523559a\",\n\"createdAt\":\"2024-01-17T09:27:46Z\",\n\"firstTimestamp\":\"2024-01-17T09:27:46Z\",\n\"lastTimestamp\":\"2024-01-17T09:27:46Z\",\n\"reason\":\"SubmittedServeDeployment\",\n\"message\":\"Controller sent API request to update Serve deployments on cluster test-v2-raycluster-lbl9x\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2.17ab185f245800ff\",\n\"name\":\"test-v2-test-v2.17ab185f245800ff\",\n\"createdAt\":\"2024-01-17T09:28:00Z\",\n\"firstTimestamp\":\"2024-01-17T09:28:00Z\",\n\"lastTimestamp\":\"2024-01-17T09:28:11Z\",\n\"reason\":\"Running\",\n\"message\":\"The Serve applicaton is now running and healthy.\",\n\"type\":\"Normal\",\n\"count\":9\n},\n{\n\"id\":\"test-v2.17ab189170a9c462\",\n\"name\":\"test-v2-test-v2.17ab189170a9c462\",\n\"createdAt\":\"2024-01-17T09:31:36Z\",\n\"firstTimestamp\":\"2024-01-17T09:31:36Z\",\n\"lastTimestamp\":\"2024-01-17T09:32:08Z\",\n\"reason\":\"ServiceNotReady\",\n\"message\":\"The service is not ready yet. Controller will perform a round of actions in 2s.\",\n\"type\":\"Normal\",\n\"count\":16\n},\n{\n\"id\":\"test-v2.17ab18965aab5e92\",\n\"name\":\"test-v2-test-v2.17ab18965aab5e92\",\n\"createdAt\":\"2024-01-17T09:31:57Z\",\n\"firstTimestamp\":\"2024-01-17T09:31:57Z\",\n\"lastTimestamp\":\"2024-01-17T09:31:57Z\",\n\"reason\":\"SubmittedServeDeployment\",\n\"message\":\"Controller sent API request to update Serve deployments on cluster test-v2-raycluster-qhrmk\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2.17ab18993b7ef94e\",\n\"name\":\"test-v2-test-v2.17ab18993b7ef94e\",\n\"createdAt\":\"2024-01-17T09:32:10Z\",\n\"firstTimestamp\":\"2024-01-17T09:32:10Z\",\n\"lastTimestamp\":\"2024-01-17T09:36:36Z\",\n\"reason\":\"Running\",\n\"message\":\"The Serve applicaton is now running and healthy.\",\n\"type\":\"Normal\",\n\"count\":135\n},\n{\n\"id\":\"test-v2-raycluster-qhrmk.17ab1891669f9337\",\n\"name\":\"test-v2-test-v2-raycluster-qhrmk.17ab1891669f9337\",\n\"createdAt\":\"2024-01-17T09:31:36Z\",\n\"firstTimestamp\":\"2024-01-17T09:31:36Z\",\n\"lastTimestamp\":\"2024-01-17T09:31:36Z\",\n\"reason\":\"Created\",\n\"message\":\"Created service test-v2-raycluster-qhrmk-head-svc\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2-raycluster-qhrmk.17ab1891683f5579\",\n\"name\":\"test-v2-test-v2-raycluster-qhrmk.17ab1891683f5579\",\n\"createdAt\":\"2024-01-17T09:31:36Z\",\n\"firstTimestamp\":\"2024-01-17T09:31:36Z\",\n\"lastTimestamp\":\"2024-01-17T09:31:36Z\",\n\"reason\":\"Created\",\n\"message\":\"Created head pod test-v2-raycluster-qhrmk-head-8kptm\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2-raycluster-qhrmk.17ab18916aa9fca3\",\n\"name\":\"test-v2-test-v2-raycluster-qhrmk.17ab18916aa9fca3\",\n\"createdAt\":\"2024-01-17T09:31:36Z\",\n\"firstTimestamp\":\"2024-01-17T09:31:36Z\",\n\"lastTimestamp\":\"2024-01-17T09:31:36Z\",\n\"reason\":\"Created\",\n\"message\":\"Created worker pod \",\n\"type\":\"Normal\",\n\"count\":1\n}\n],\n\"rayClusterName\":\"test-v2-raycluster-qhrmk\",\n\"serveApplicationStatus\":[\n{\n\"name\":\"fruit_app\",\n\"status\":\"RUNNING\",\n\"serveDeploymentStatus\":[\n{\n\"deploymentName\":\"PearStand\",\n\"status\":\"HEALTHY\"\n},\n{\n\"deploymentName\":\"FruitMarket\",\n\"status\":\"HEALTHY\"\n},\n{\n\"deploymentName\":\"MangoStand\",\n\"status\":\"HEALTHY\"\n},\n{\n\"deploymentName\":\"OrangeStand\",\n\"status\":\"HEALTHY\"\n}\n]\n},\n{\n\"name\":\"math_app\",\n\"status\":\"RUNNING\",\n\"serveDeploymentStatus\":[\n{\n\"deploymentName\":\"Adder\",\n\"status\":\"HEALTHY\"\n},\n{\n\"deploymentName\":\"Multiplier\",\n\"status\":\"HEALTHY\"\n},\n{\n\"deploymentName\":\"Router\",\n\"status\":\"HEALTHY\"\n}\n]\n}\n]\n},\n\"createdAt\":\"2024-01-17T09:31:34Z\",\n\"deleteAt\":\"1969-12-31T23:59:59Z\"\n}\n]\n}\n</code></pre>"},{"location":"components/apiserver/#list-all-services-in-all-namespaces","title":"List all services in all namespaces","text":"<pre><code>GET {{baseUrl}}/apis/v1/services\n</code></pre> <p>Examples:</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'GET' \\\n'http://localhost:31888/apis/v1/services' \\\n-H 'accept: application/json'\n</code></pre> <ul> <li>Response:</li> </ul> <pre><code>{\n\"services\":[\n{\n\"name\":\"test-v2\",\n\"namespace\":\"default\",\n\"user\":\"user\",\n\"serveConfigV2\":\"applications:\\n  - name: fruit_app\\n    import_path: fruit.deployment_graph\\n    route_prefix: /fruit\\n    runtime_env:\\n      working_dir: \\\"https://github.com/ray-project/test_dag/archive/78b4a5da38796123d9f9ffff59bab2792a043e95.zip\\\"\\n    deployments:\\n      - name: MangoStand\\n        num_replicas: 2\\n        max_replicas_per_node: 1\\n        user_config:\\n          price: 3\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: OrangeStand\\n        num_replicas: 1\\n        user_config:\\n          price: 2\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: PearStand\\n        num_replicas: 1\\n        user_config:\\n          price: 1\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: FruitMarket\\n        num_replicas: 1\\n        ray_actor_options:\\n          num_cpus: 0.1\\n  - name: math_app\\n    import_path: conditional_dag.serve_dag\\n    route_prefix: /calc\\n    runtime_env:\\n      working_dir: \\\"https://github.com/ray-project/test_dag/archive/78b4a5da38796123d9f9ffff59bab2792a043e95.zip\\\"\\n    deployments:\\n      - name: Adder\\n        num_replicas: 1\\n        user_config:\\n          increment: 3\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: Multiplier\\n        num_replicas: 1\\n        user_config:\\n          factor: 5\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: Router\\n        num_replicas: 1\\n\",\n\"clusterSpec\":{\n\"headGroupSpec\":{\n\"computeTemplate\":\"default-template\",\n\"image\":\"rayproject/ray:2.9.0-py310\",\n\"serviceType\":\"NodePort\",\n\"rayStartParams\":{\n\"dashboard-host\":\"0.0.0.0\",\n\"metrics-export-port\":\"8080\"\n},\n\"environment\":{\n\n}\n},\n\"workerGroupSpec\":[\n{\n\"groupName\":\"small-wg\",\n\"computeTemplate\":\"default-template\",\n\"image\":\"rayproject/ray:2.9.0-py310\",\n\"replicas\":1,\n\"minReplicas\":1,\n\"maxReplicas\":5,\n\"rayStartParams\":{\n\"node-ip-address\":\"$MY_POD_IP\"\n},\n\"environment\":{\n\n}\n}\n]\n},\n\"rayServiceStatus\":{\n\"rayServiceEvents\":[\n{\n\"id\":\"test-v2.17ab175ec787d26e\",\n\"name\":\"test-v2-test-v2.17ab175ec787d26e\",\n\"createdAt\":\"2024-01-17T09:09:39Z\",\n\"firstTimestamp\":\"2024-01-17T09:09:39Z\",\n\"lastTimestamp\":\"2024-01-17T09:10:04Z\",\n\"reason\":\"ServiceNotReady\",\n\"message\":\"The service is not ready yet. Controller will perform a round of actions in 2s.\",\n\"type\":\"Normal\",\n\"count\":13\n},\n{\n\"id\":\"test-v2.17ab176292337141\",\n\"name\":\"test-v2-test-v2.17ab176292337141\",\n\"createdAt\":\"2024-01-17T09:09:56Z\",\n\"firstTimestamp\":\"2024-01-17T09:09:56Z\",\n\"lastTimestamp\":\"2024-01-17T09:09:56Z\",\n\"reason\":\"SubmittedServeDeployment\",\n\"message\":\"Controller sent API request to update Serve deployments on cluster test-v2-raycluster-lmk5c\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2.17ab1764fe827fe3\",\n\"name\":\"test-v2-test-v2.17ab1764fe827fe3\",\n\"createdAt\":\"2024-01-17T09:10:06Z\",\n\"firstTimestamp\":\"2024-01-17T09:10:06Z\",\n\"lastTimestamp\":\"2024-01-17T09:14:41Z\",\n\"reason\":\"Running\",\n\"message\":\"The Serve applicaton is now running and healthy.\",\n\"type\":\"Normal\",\n\"count\":139\n},\n{\n\"id\":\"test-v2.17ab1815ce6a98da\",\n\"name\":\"test-v2-test-v2.17ab1815ce6a98da\",\n\"createdAt\":\"2024-01-17T09:22:45Z\",\n\"firstTimestamp\":\"2024-01-17T09:22:45Z\",\n\"lastTimestamp\":\"2024-01-17T09:23:16Z\",\n\"reason\":\"ServiceNotReady\",\n\"message\":\"The service is not ready yet. Controller will perform a round of actions in 2s.\",\n\"type\":\"Normal\",\n\"count\":16\n},\n{\n\"id\":\"test-v2.17ab181a8576812d\",\n\"name\":\"test-v2-test-v2.17ab181a8576812d\",\n\"createdAt\":\"2024-01-17T09:23:06Z\",\n\"firstTimestamp\":\"2024-01-17T09:23:06Z\",\n\"lastTimestamp\":\"2024-01-17T09:23:06Z\",\n\"reason\":\"SubmittedServeDeployment\",\n\"message\":\"Controller sent API request to update Serve deployments on cluster test-v2-raycluster-b85bj\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2.17ab181d64327965\",\n\"name\":\"test-v2-test-v2.17ab181d64327965\",\n\"createdAt\":\"2024-01-17T09:23:18Z\",\n\"firstTimestamp\":\"2024-01-17T09:23:18Z\",\n\"lastTimestamp\":\"2024-01-17T09:23:26Z\",\n\"reason\":\"Running\",\n\"message\":\"The Serve applicaton is now running and healthy.\",\n\"type\":\"Normal\",\n\"count\":8\n},\n{\n\"id\":\"test-v2.17ab1857fcf7e3a4\",\n\"name\":\"test-v2-test-v2.17ab1857fcf7e3a4\",\n\"createdAt\":\"2024-01-17T09:27:30Z\",\n\"firstTimestamp\":\"2024-01-17T09:27:30Z\",\n\"lastTimestamp\":\"2024-01-17T09:27:58Z\",\n\"reason\":\"ServiceNotReady\",\n\"message\":\"The service is not ready yet. Controller will perform a round of actions in 2s.\",\n\"type\":\"Normal\",\n\"count\":15\n},\n{\n\"id\":\"test-v2.17ab185bc523559a\",\n\"name\":\"test-v2-test-v2.17ab185bc523559a\",\n\"createdAt\":\"2024-01-17T09:27:46Z\",\n\"firstTimestamp\":\"2024-01-17T09:27:46Z\",\n\"lastTimestamp\":\"2024-01-17T09:27:46Z\",\n\"reason\":\"SubmittedServeDeployment\",\n\"message\":\"Controller sent API request to update Serve deployments on cluster test-v2-raycluster-lbl9x\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2.17ab185f245800ff\",\n\"name\":\"test-v2-test-v2.17ab185f245800ff\",\n\"createdAt\":\"2024-01-17T09:28:00Z\",\n\"firstTimestamp\":\"2024-01-17T09:28:00Z\",\n\"lastTimestamp\":\"2024-01-17T09:28:11Z\",\n\"reason\":\"Running\",\n\"message\":\"The Serve applicaton is now running and healthy.\",\n\"type\":\"Normal\",\n\"count\":9\n},\n{\n\"id\":\"test-v2.17ab189170a9c462\",\n\"name\":\"test-v2-test-v2.17ab189170a9c462\",\n\"createdAt\":\"2024-01-17T09:31:36Z\",\n\"firstTimestamp\":\"2024-01-17T09:31:36Z\",\n\"lastTimestamp\":\"2024-01-17T09:32:08Z\",\n\"reason\":\"ServiceNotReady\",\n\"message\":\"The service is not ready yet. Controller will perform a round of actions in 2s.\",\n\"type\":\"Normal\",\n\"count\":16\n},\n{\n\"id\":\"test-v2.17ab18965aab5e92\",\n\"name\":\"test-v2-test-v2.17ab18965aab5e92\",\n\"createdAt\":\"2024-01-17T09:31:57Z\",\n\"firstTimestamp\":\"2024-01-17T09:31:57Z\",\n\"lastTimestamp\":\"2024-01-17T09:31:57Z\",\n\"reason\":\"SubmittedServeDeployment\",\n\"message\":\"Controller sent API request to update Serve deployments on cluster test-v2-raycluster-qhrmk\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2.17ab18993b7ef94e\",\n\"name\":\"test-v2-test-v2.17ab18993b7ef94e\",\n\"createdAt\":\"2024-01-17T09:32:10Z\",\n\"firstTimestamp\":\"2024-01-17T09:32:10Z\",\n\"lastTimestamp\":\"2024-01-17T09:41:37Z\",\n\"reason\":\"Running\",\n\"message\":\"The Serve applicaton is now running and healthy.\",\n\"type\":\"Normal\",\n\"count\":284\n},\n{\n\"id\":\"test-v2-raycluster-qhrmk.17ab1891669f9337\",\n\"name\":\"test-v2-test-v2-raycluster-qhrmk.17ab1891669f9337\",\n\"createdAt\":\"2024-01-17T09:31:36Z\",\n\"firstTimestamp\":\"2024-01-17T09:31:36Z\",\n\"lastTimestamp\":\"2024-01-17T09:31:36Z\",\n\"reason\":\"Created\",\n\"message\":\"Created service test-v2-raycluster-qhrmk-head-svc\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2-raycluster-qhrmk.17ab1891683f5579\",\n\"name\":\"test-v2-test-v2-raycluster-qhrmk.17ab1891683f5579\",\n\"createdAt\":\"2024-01-17T09:31:36Z\",\n\"firstTimestamp\":\"2024-01-17T09:31:36Z\",\n\"lastTimestamp\":\"2024-01-17T09:31:36Z\",\n\"reason\":\"Created\",\n\"message\":\"Created head pod test-v2-raycluster-qhrmk-head-8kptm\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2-raycluster-qhrmk.17ab18916aa9fca3\",\n\"name\":\"test-v2-test-v2-raycluster-qhrmk.17ab18916aa9fca3\",\n\"createdAt\":\"2024-01-17T09:31:36Z\",\n\"firstTimestamp\":\"2024-01-17T09:31:36Z\",\n\"lastTimestamp\":\"2024-01-17T09:31:36Z\",\n\"reason\":\"Created\",\n\"message\":\"Created worker pod \",\n\"type\":\"Normal\",\n\"count\":1\n}\n],\n\"rayClusterName\":\"test-v2-raycluster-qhrmk\",\n\"serveApplicationStatus\":[\n{\n\"name\":\"fruit_app\",\n\"status\":\"RUNNING\",\n\"serveDeploymentStatus\":[\n{\n\"deploymentName\":\"FruitMarket\",\n\"status\":\"HEALTHY\"\n},\n{\n\"deploymentName\":\"MangoStand\",\n\"status\":\"HEALTHY\"\n},\n{\n\"deploymentName\":\"OrangeStand\",\n\"status\":\"HEALTHY\"\n},\n{\n\"deploymentName\":\"PearStand\",\n\"status\":\"HEALTHY\"\n}\n]\n},\n{\n\"name\":\"math_app\",\n\"status\":\"RUNNING\",\n\"serveDeploymentStatus\":[\n{\n\"deploymentName\":\"Adder\",\n\"status\":\"HEALTHY\"\n},\n{\n\"deploymentName\":\"Multiplier\",\n\"status\":\"HEALTHY\"\n},\n{\n\"deploymentName\":\"Router\",\n\"status\":\"HEALTHY\"\n}\n]\n}\n]\n},\n\"createdAt\":\"2024-01-17T09:31:34Z\",\n\"deleteAt\":\"1969-12-31T23:59:59Z\"\n}\n]\n}\n</code></pre>"},{"location":"components/apiserver/#get-service-by-its-name-and-namespace","title":"Get service by its name and namespace","text":"<pre><code>GET {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/services/&lt;service_name&gt;\n</code></pre> <p>Examples:</p> <ul> <li>Request:</li> </ul> <pre><code>curl --silent -X 'GET' \\\n'http://localhost:31888/apis/v1/namespaces/default/services/test-v2' \\\n-H 'accept: application/json'  </code></pre> <ul> <li>Response:</li> </ul> <pre><code>{\n\"name\":\"test-v2\",\n\"namespace\":\"default\",\n\"user\":\"user\",\n\"serveConfigV2\":\"applications:\\n  - name: fruit_app\\n    import_path: fruit.deployment_graph\\n    route_prefix: /fruit\\n    runtime_env:\\n      working_dir: \\\"https://github.com/ray-project/test_dag/archive/78b4a5da38796123d9f9ffff59bab2792a043e95.zip\\\"\\n    deployments:\\n      - name: MangoStand\\n        num_replicas: 2\\n        max_replicas_per_node: 1\\n        user_config:\\n          price: 3\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: OrangeStand\\n        num_replicas: 1\\n        user_config:\\n          price: 2\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: PearStand\\n        num_replicas: 1\\n        user_config:\\n          price: 1\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: FruitMarket\\n        num_replicas: 1\\n        ray_actor_options:\\n          num_cpus: 0.1\\n  - name: math_app\\n    import_path: conditional_dag.serve_dag\\n    route_prefix: /calc\\n    runtime_env:\\n      working_dir: \\\"https://github.com/ray-project/test_dag/archive/78b4a5da38796123d9f9ffff59bab2792a043e95.zip\\\"\\n    deployments:\\n      - name: Adder\\n        num_replicas: 1\\n        user_config:\\n          increment: 3\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: Multiplier\\n        num_replicas: 1\\n        user_config:\\n          factor: 5\\n        ray_actor_options:\\n          num_cpus: 0.1\\n      - name: Router\\n        num_replicas: 1\\n\",\n\"clusterSpec\":{\n\"headGroupSpec\":{\n\"computeTemplate\":\"default-template\",\n\"image\":\"rayproject/ray:2.9.0-py310\",\n\"serviceType\":\"NodePort\",\n\"rayStartParams\":{\n\"dashboard-host\":\"0.0.0.0\",\n\"metrics-export-port\":\"8080\"\n},\n\"environment\":{\n\n}\n},\n\"workerGroupSpec\":[\n{\n\"groupName\":\"small-wg\",\n\"computeTemplate\":\"default-template\",\n\"image\":\"rayproject/ray:2.9.0-py310\",\n\"replicas\":1,\n\"minReplicas\":1,\n\"maxReplicas\":5,\n\"rayStartParams\":{\n\"node-ip-address\":\"$MY_POD_IP\"\n},\n\"environment\":{\n\n}\n}\n]\n},\n\"rayServiceStatus\":{\n\"rayServiceEvents\":[\n{\n\"id\":\"test-v2.17ab175ec787d26e\",\n\"name\":\"test-v2-test-v2.17ab175ec787d26e\",\n\"createdAt\":\"2024-01-17T09:09:39Z\",\n\"firstTimestamp\":\"2024-01-17T09:09:39Z\",\n\"lastTimestamp\":\"2024-01-17T09:10:04Z\",\n\"reason\":\"ServiceNotReady\",\n\"message\":\"The service is not ready yet. Controller will perform a round of actions in 2s.\",\n\"type\":\"Normal\",\n\"count\":13\n},\n{\n\"id\":\"test-v2.17ab176292337141\",\n\"name\":\"test-v2-test-v2.17ab176292337141\",\n\"createdAt\":\"2024-01-17T09:09:56Z\",\n\"firstTimestamp\":\"2024-01-17T09:09:56Z\",\n\"lastTimestamp\":\"2024-01-17T09:09:56Z\",\n\"reason\":\"SubmittedServeDeployment\",\n\"message\":\"Controller sent API request to update Serve deployments on cluster test-v2-raycluster-lmk5c\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2.17ab1764fe827fe3\",\n\"name\":\"test-v2-test-v2.17ab1764fe827fe3\",\n\"createdAt\":\"2024-01-17T09:10:06Z\",\n\"firstTimestamp\":\"2024-01-17T09:10:06Z\",\n\"lastTimestamp\":\"2024-01-17T09:14:41Z\",\n\"reason\":\"Running\",\n\"message\":\"The Serve applicaton is now running and healthy.\",\n\"type\":\"Normal\",\n\"count\":139\n},\n{\n\"id\":\"test-v2.17ab1815ce6a98da\",\n\"name\":\"test-v2-test-v2.17ab1815ce6a98da\",\n\"createdAt\":\"2024-01-17T09:22:45Z\",\n\"firstTimestamp\":\"2024-01-17T09:22:45Z\",\n\"lastTimestamp\":\"2024-01-17T09:23:16Z\",\n\"reason\":\"ServiceNotReady\",\n\"message\":\"The service is not ready yet. Controller will perform a round of actions in 2s.\",\n\"type\":\"Normal\",\n\"count\":16\n},\n{\n\"id\":\"test-v2.17ab181a8576812d\",\n\"name\":\"test-v2-test-v2.17ab181a8576812d\",\n\"createdAt\":\"2024-01-17T09:23:06Z\",\n\"firstTimestamp\":\"2024-01-17T09:23:06Z\",\n\"lastTimestamp\":\"2024-01-17T09:23:06Z\",\n\"reason\":\"SubmittedServeDeployment\",\n\"message\":\"Controller sent API request to update Serve deployments on cluster test-v2-raycluster-b85bj\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2.17ab181d64327965\",\n\"name\":\"test-v2-test-v2.17ab181d64327965\",\n\"createdAt\":\"2024-01-17T09:23:18Z\",\n\"firstTimestamp\":\"2024-01-17T09:23:18Z\",\n\"lastTimestamp\":\"2024-01-17T09:23:26Z\",\n\"reason\":\"Running\",\n\"message\":\"The Serve applicaton is now running and healthy.\",\n\"type\":\"Normal\",\n\"count\":8\n},\n{\n\"id\":\"test-v2.17ab1857fcf7e3a4\",\n\"name\":\"test-v2-test-v2.17ab1857fcf7e3a4\",\n\"createdAt\":\"2024-01-17T09:27:30Z\",\n\"firstTimestamp\":\"2024-01-17T09:27:30Z\",\n\"lastTimestamp\":\"2024-01-17T09:27:58Z\",\n\"reason\":\"ServiceNotReady\",\n\"message\":\"The service is not ready yet. Controller will perform a round of actions in 2s.\",\n\"type\":\"Normal\",\n\"count\":15\n},\n{\n\"id\":\"test-v2.17ab185bc523559a\",\n\"name\":\"test-v2-test-v2.17ab185bc523559a\",\n\"createdAt\":\"2024-01-17T09:27:46Z\",\n\"firstTimestamp\":\"2024-01-17T09:27:46Z\",\n\"lastTimestamp\":\"2024-01-17T09:27:46Z\",\n\"reason\":\"SubmittedServeDeployment\",\n\"message\":\"Controller sent API request to update Serve deployments on cluster test-v2-raycluster-lbl9x\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2.17ab185f245800ff\",\n\"name\":\"test-v2-test-v2.17ab185f245800ff\",\n\"createdAt\":\"2024-01-17T09:28:00Z\",\n\"firstTimestamp\":\"2024-01-17T09:28:00Z\",\n\"lastTimestamp\":\"2024-01-17T09:28:11Z\",\n\"reason\":\"Running\",\n\"message\":\"The Serve applicaton is now running and healthy.\",\n\"type\":\"Normal\",\n\"count\":9\n},\n{\n\"id\":\"test-v2.17ab189170a9c462\",\n\"name\":\"test-v2-test-v2.17ab189170a9c462\",\n\"createdAt\":\"2024-01-17T09:31:36Z\",\n\"firstTimestamp\":\"2024-01-17T09:31:36Z\",\n\"lastTimestamp\":\"2024-01-17T09:32:08Z\",\n\"reason\":\"ServiceNotReady\",\n\"message\":\"The service is not ready yet. Controller will perform a round of actions in 2s.\",\n\"type\":\"Normal\",\n\"count\":16\n},\n{\n\"id\":\"test-v2.17ab18965aab5e92\",\n\"name\":\"test-v2-test-v2.17ab18965aab5e92\",\n\"createdAt\":\"2024-01-17T09:31:57Z\",\n\"firstTimestamp\":\"2024-01-17T09:31:57Z\",\n\"lastTimestamp\":\"2024-01-17T09:31:57Z\",\n\"reason\":\"SubmittedServeDeployment\",\n\"message\":\"Controller sent API request to update Serve deployments on cluster test-v2-raycluster-qhrmk\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2.17ab18993b7ef94e\",\n\"name\":\"test-v2-test-v2.17ab18993b7ef94e\",\n\"createdAt\":\"2024-01-17T09:32:10Z\",\n\"firstTimestamp\":\"2024-01-17T09:32:10Z\",\n\"lastTimestamp\":\"2024-01-17T09:46:38Z\",\n\"reason\":\"Running\",\n\"message\":\"The Serve applicaton is now running and healthy.\",\n\"type\":\"Normal\",\n\"count\":433\n},\n{\n\"id\":\"test-v2-raycluster-qhrmk.17ab1891669f9337\",\n\"name\":\"test-v2-test-v2-raycluster-qhrmk.17ab1891669f9337\",\n\"createdAt\":\"2024-01-17T09:31:36Z\",\n\"firstTimestamp\":\"2024-01-17T09:31:36Z\",\n\"lastTimestamp\":\"2024-01-17T09:31:36Z\",\n\"reason\":\"Created\",\n\"message\":\"Created service test-v2-raycluster-qhrmk-head-svc\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2-raycluster-qhrmk.17ab1891683f5579\",\n\"name\":\"test-v2-test-v2-raycluster-qhrmk.17ab1891683f5579\",\n\"createdAt\":\"2024-01-17T09:31:36Z\",\n\"firstTimestamp\":\"2024-01-17T09:31:36Z\",\n\"lastTimestamp\":\"2024-01-17T09:31:36Z\",\n\"reason\":\"Created\",\n\"message\":\"Created head pod test-v2-raycluster-qhrmk-head-8kptm\",\n\"type\":\"Normal\",\n\"count\":1\n},\n{\n\"id\":\"test-v2-raycluster-qhrmk.17ab18916aa9fca3\",\n\"name\":\"test-v2-test-v2-raycluster-qhrmk.17ab18916aa9fca3\",\n\"createdAt\":\"2024-01-17T09:31:36Z\",\n\"firstTimestamp\":\"2024-01-17T09:31:36Z\",\n\"lastTimestamp\":\"2024-01-17T09:31:36Z\",\n\"reason\":\"Created\",\n\"message\":\"Created worker pod \",\n\"type\":\"Normal\",\n\"count\":1\n}\n],\n\"rayClusterName\":\"test-v2-raycluster-qhrmk\",\n\"serveApplicationStatus\":[\n{\n\"name\":\"fruit_app\",\n\"status\":\"RUNNING\",\n\"serveDeploymentStatus\":[\n{\n\"deploymentName\":\"PearStand\",\n\"status\":\"HEALTHY\"\n},\n{\n\"deploymentName\":\"FruitMarket\",\n\"status\":\"HEALTHY\"\n},\n{\n\"deploymentName\":\"MangoStand\",\n\"status\":\"HEALTHY\"\n},\n{\n\"deploymentName\":\"OrangeStand\",\n\"status\":\"HEALTHY\"\n}\n]\n},\n{\n\"name\":\"math_app\",\n\"status\":\"RUNNING\",\n\"serveDeploymentStatus\":[\n{\n\"deploymentName\":\"Adder\",\n\"status\":\"HEALTHY\"\n},\n{\n\"deploymentName\":\"Multiplier\",\n\"status\":\"HEALTHY\"\n},\n{\n\"deploymentName\":\"Router\",\n\"status\":\"HEALTHY\"\n}\n]\n}\n]\n},\n\"createdAt\":\"2024-01-17T09:31:34Z\",\n\"deleteAt\":\"1969-12-31T23:59:59Z\"\n}\n</code></pre>"},{"location":"components/apiserver/#delete-service-by-its-name-and-namespace","title":"Delete service by its name and namespace","text":"<pre><code>DELETE {{baseUrl}}/apis/v1/namespaces/&lt;namespace&gt;/services/&lt;service_name&gt;\n</code></pre> <p>Examples:</p> <ul> <li>Request</li> </ul> <pre><code>curl --silent -X 'DELETE' \\\n'http://localhost:31888/apis/v1/namespaces/default/services/test-v2' \\\n-H 'accept: application/json'  </code></pre> <ul> <li>Response</li> </ul> <pre><code>{}\n</code></pre>"},{"location":"components/cli/","title":"KubeRay CLI","text":"<p>KubeRay CLI provides the ability to manage kuberay resources (ray clusters, compute templates etc) through command line interface.</p> <p>Note</p> <p>The KubeRay CLI is an optional interface backed by the KubeRay API server. It provides a layer of simplified configuration for KubeRay resources.</p> <p>The KubeRay CLI is community-managed and is not officially endorsed by the Ray maintainers. At this time, the only officially supported methods for managing KubeRay resources are</p> <ul> <li>Direct management of KubeRay custom resources via kubectl, kustomize, and Kubernetes language clients.</li> <li>Helm charts.</li> </ul> <p>KubeRay CLI maintainer contacts (GitHub handles): @Jeffwan @scarlet25151</p>"},{"location":"components/cli/#installation","title":"Installation","text":"<p>Please check release page and download the binaries. </p>"},{"location":"components/cli/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kuberay operator needs to be running.</li> <li>Kuberay apiserver needs to be running and accessible.</li> </ul>"},{"location":"components/cli/#development","title":"Development","text":"<ul> <li>Kuberay CLI uses Cobra framework for the CLI application.</li> <li>Kuberay CLI depends on kuberay apiserver to manage these resources by sending grpc requests to the kuberay apiserver.</li> </ul> <p>You can build kuberay binary following this way.</p> <pre><code>cd kuberay/cli\ngo build -o kuberay -a main.go\n</code></pre>"},{"location":"components/cli/#usage","title":"Usage","text":""},{"location":"components/cli/#configure-kuberay-apiserver-endpoint","title":"Configure kuberay apiserver endpoint","text":"<ul> <li>Default kuberay apiserver endpoint: <code>127.0.0.1:8887</code>.</li> <li>If kuberay apiserver is not run locally, this must be set in order to manage ray clusters and ray compute templates.</li> </ul>"},{"location":"components/cli/#read-current-kuberay-apiserver-endpoint","title":"Read current kuberay apiserver endpoint","text":"<p><code>./kuberay config get endpoint</code></p>"},{"location":"components/cli/#reset-kuberay-apiserver-endpoint-to-default-1270018887","title":"Reset kuberay apiserver endpoint to default (<code>127.0.0.1:8887</code>)","text":"<p><code>./kuberay config reset endpoint</code></p>"},{"location":"components/cli/#set-kuberay-apiserver-endpoint","title":"Set kuberay apiserver endpoint","text":"<p><code>./kuberay config set endpoint &lt;kuberay apiserver endpoint&gt;</code></p>"},{"location":"components/cli/#manage-ray-clusters","title":"Manage Ray Clusters","text":""},{"location":"components/cli/#create-a-ray-cluster","title":"Create a Ray Cluster","text":"<pre><code>Usage:\nkuberay cluster create [flags]\n\nFlags:\n      --environment string               environment of the cluster (valid values: DEV, TESTING, STAGING, PRODUCTION) (default \"DEV\")\n      --head-compute-template string     compute template name for ray head\n      --head-image string                ray head image\n      --head-service-type string         ray head service type (ClusterIP, NodePort, LoadBalancer) (default \"ClusterIP\")\n      --name string                      name of the cluster\n  -n, --namespace string                 kubernetes namespace where the cluster will be\n      --user string                      SSO username of ray cluster creator\n      --version string                   version of the ray cluster (default \"1.9.0\")\n      --worker-compute-template string   compute template name of worker in the first worker group\n      --worker-group-name string         first worker group name\n      --worker-image string              image of worker in the first worker group\n      --worker-replicas uint32           pod replicas of workers in the first worker group (default 1)\n</code></pre> <p>Known Limitation: Currently only one worker compute template is supported during creation. </p>"},{"location":"components/cli/#get-a-ray-cluster","title":"Get a Ray Cluster","text":"<p><code>./kuberay cluster get -n &lt;namespace&gt; &lt;cluster name&gt;</code></p>"},{"location":"components/cli/#list-ray-clusters","title":"List Ray Clusters","text":"<p><code>./kuberay cluster -n &lt;namespace&gt; list</code></p>"},{"location":"components/cli/#delete-a-ray-cluster","title":"Delete a Ray Cluster","text":"<p><code>./kuberay cluster delete -n &lt;namespace&gt; &lt;cluster name&gt;</code></p>"},{"location":"components/cli/#manage-ray-compute-template","title":"Manage Ray Compute Template","text":""},{"location":"components/cli/#create-a-compute-template","title":"Create a Compute Template","text":"<pre><code>Usage:\n  kuberay template compute create [flags]\n\nFlags:\n      --cpu uint32               ray pod CPU (default 1)\n      --gpu uint32               ray head GPU\n      --gpu-accelerator string   GPU Accelerator type\n      --memory uint32            ray pod memory in GB (default 1)\n      --name string              name of the compute template\n  -n, --namespace string         kubernetes namespace where the compute template will be stored\n</code></pre>"},{"location":"components/cli/#get-a-ray-compute-template","title":"Get a Ray Compute Template","text":"<p><code>./kuberay template compute get -n &lt;namespace&gt; &lt;compute template name&gt;</code></p>"},{"location":"components/cli/#list-ray-compute-templates","title":"List Ray Compute Templates","text":"<p><code>./kuberay template compute list -n &lt;namespace&gt;</code></p>"},{"location":"components/cli/#delete-a-ray-compute-template","title":"Delete a Ray Compute Template","text":"<p><code>./kuberay template compute delete -n &lt;namespace&gt; &lt;compute template name&gt;</code></p>"},{"location":"components/cli/#end-to-end-example","title":"End to end example","text":"<p>Configure the endpoints</p> <pre><code>kubectl port-forward svc/kuberay-apiserver-service 8887:8887 -n ray-system\n./kuberay config set endpoint 127.0.0.1:8887\n</code></pre> <p>Create compute templates</p> <pre><code>./kuberay template compute create -n &lt;namespace&gt; --cpu 2 --memory 4 --name \"worker-template\"\n./kuberay template compute create -n &lt;namespace&gt; --cpu 1 --memory 2 --name \"head-template\"\n</code></pre> <p>List compute templates created</p> <pre><code>./kuberay template compute list\n</code></pre> <p>Create the cluster</p> <pre><code>./kuberay cluster create -n &lt;namespace&gt; --name test-cluster --user jiaxin.shan \\\n--head-compute-template head-template \\\n--head-image rayproject/ray:1.9.2 \\\n--worker-group-name small-wg \\\n--worker-compute-template worker-template \\\n--worker-image rayproject/ray:1.9.2\n</code></pre> <p>List the clusters</p> <pre><code>./kuberay cluster list\n</code></pre>"},{"location":"components/operator/","title":"KubeRay Operator","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"deploy/helm-cluster/","title":"RayCluster","text":"<p>RayCluster is a custom resource definition (CRD). KubeRay operator will listen to the resource events about RayCluster and create related Kubernetes resources (e.g. Pod &amp; Service). Hence, KubeRay operator installation and CRD registration are required for this guide.</p>"},{"location":"deploy/helm-cluster/#prerequisites","title":"Prerequisites","text":"<p>See kuberay-operator/README.md for more details. * Helm * Install custom resource definition and KubeRay operator (covered by the following end-to-end example.)</p>"},{"location":"deploy/helm-cluster/#end-to-end-example","title":"End-to-end example","text":"<pre><code># Step 1: Create a KinD cluster \nkind create cluster\n\n# Step 2: Register a Helm chart repo\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\n\n# Step 3: Install both CRDs and KubeRay operator v1.0.0.\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.0.0\n\n# Step 4: Install a RayCluster custom resource\n# (For x86_64 users)\nhelm install raycluster kuberay/ray-cluster --version 1.0.0\n# (For arm64 users, e.g. Mac M1)\n# See here for all available arm64 images: https://hub.docker.com/r/rayproject/ray/tags?page=1&amp;name=aarch64\nhelm install raycluster kuberay/ray-cluster --version 1.0.0 --set image.tag=nightly-aarch64\n\n# Step 5: Verify the installation of KubeRay operator and RayCluster \nkubectl get pods\n# NAME                                          READY   STATUS    RESTARTS   AGE\n# kuberay-operator-6fcbb94f64-gkpc9             1/1     Running   0          89s\n# raycluster-kuberay-head-qp9f4                 1/1     Running   0          66s\n# raycluster-kuberay-worker-workergroup-2jckt   1/1     Running   0          66s\n\n# Step 6: Forward the port of Dashboard\nkubectl port-forward --address 0.0.0.0 svc/raycluster-kuberay-head-svc 8265:8265\n\n# Step 7: Check ${YOUR_IP}:8265 for the Dashboard (e.g. 127.0.0.1:8265)\n\n# Step 8: Log in to Ray head Pod and execute a job.\nkubectl exec -it ${RAYCLUSTER_HEAD_POD} -- bash\npython -c \"import ray; ray.init(); print(ray.cluster_resources())\" # (in Ray head Pod)\n\n# Step 9: Check ${YOUR_IP}:8265/#/job. The status of the job should be \"SUCCEEDED\".\n\n# Step 10: Uninstall RayCluster\nhelm uninstall raycluster\n\n# Step 11: Verify that RayCluster has been removed successfully\n# NAME                                READY   STATUS    RESTARTS   AGE\n# kuberay-operator-6fcbb94f64-gkpc9   1/1     Running   0          9m57s\n</code></pre>"},{"location":"deploy/helm/","title":"KubeRay Operator","text":"<p>This document provides instructions to install both CRDs (RayCluster, RayJob, RayService) and KubeRay operator with a Helm chart.</p>"},{"location":"deploy/helm/#helm","title":"Helm","text":"<p>Make sure the version of Helm is v3+. Currently, existing CI tests are based on Helm v3.4.1 and v3.9.4.</p> <pre><code>helm version\n</code></pre>"},{"location":"deploy/helm/#install-crds-and-kuberay-operator","title":"Install CRDs and KubeRay operator","text":"<ul> <li> <p>Install a stable version via Helm repository (only supports KubeRay v0.4.0+)   <pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\n\n# Install both CRDs and KubeRay operator v1.0.0.\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.0.0\n\n# Check the KubeRay operator Pod in `default` namespace\nkubectl get pods\n# NAME                                READY   STATUS    RESTARTS   AGE\n# kuberay-operator-6fcbb94f64-mbfnr   1/1     Running   0          17s\n</code></pre></p> </li> <li> <p>Install the nightly version   <pre><code># Step1: Clone KubeRay repository\n\n# Step2: Move to `helm-chart/kuberay-operator`\n\n# Step3: Install KubeRay operator\nhelm install kuberay-operator .\n</code></pre></p> </li> <li> <p>Install KubeRay operator without installing CRDs</p> </li> <li>In some cases, the installation of the CRDs and the installation of the operator may require different levels of admin permissions, so these two installations could be handled as different steps by different roles.</li> <li>Use Helm's built-in <code>--skip-crds</code> flag to install the operator only. See this document for more details.   <pre><code># Step 1: Install CRDs only (for cluster admin)\nkubectl create -k \"github.com/ray-project/kuberay/ray-operator/config/crd?ref=v1.0.0&amp;timeout=90s\"\n\n# Step 2: Install KubeRay operator only. (for developer)\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.0.0 --skip-crds\n</code></pre></li> </ul>"},{"location":"deploy/helm/#list-the-chart","title":"List the chart","text":"<p>To list the <code>my-release</code> deployment:</p> <pre><code>helm ls\n# NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION\n# kuberay-operator        default         1               2023-09-22 02:57:17.306616331 +0000 UTC deployed        kuberay-operator-1.0.0\n</code></pre>"},{"location":"deploy/helm/#uninstall-the-chart","title":"Uninstall the Chart","text":"<pre><code># Uninstall the `kuberay-operator` release\nhelm uninstall kuberay-operator\n\n# The operator Pod should be removed.\nkubectl get pods\n# No resources found in default namespace.\n</code></pre>"},{"location":"deploy/helm/#working-with-argo-cd","title":"Working with Argo CD","text":"<p>If you are using Argo CD to manage the operator, you will encounter the issue which complains the CRDs too long. Same with this issue. The recommended solution is to split the operator into two Argo apps, such as:</p> <ul> <li>The first app just for installing the CRDs with <code>Replace=true</code> directly, snippet:</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\nname: ray-operator-crds\nspec:\nproject: default\nsource:\nrepoURL: https://github.com/ray-project/kuberay\ntargetRevision: v1.0.0-rc.0\npath: helm-chart/kuberay-operator/crds\ndestination:\nserver: https://kubernetes.default.svc\nsyncPolicy:\nsyncOptions:\n- Replace=true\n...\n</code></pre> <ul> <li>The second app that installs the Helm chart with <code>skipCrds=true</code> (new feature in Argo CD 2.3.0), snippet:</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\nname: ray-operator\nspec:\nsource:\nrepoURL: https://github.com/ray-project/kuberay\ntargetRevision: v1.0.0-rc.0\npath: helm-chart/kuberay-operator\nhelm:\nskipCrds: true\ndestination:\nserver: https://kubernetes.default.svc\nnamespace: ray-operator\nsyncPolicy:\nsyncOptions:\n- CreateNamespace=true\n...\n</code></pre>"},{"location":"deploy/images/","title":"Container Images","text":"<p>Images for the various KubeRay components are published at the following locations:</p> <ol> <li>Quay.io</li> <li>DockerHub</li> </ol> <p>We recommend using Quay.io as the primary source for images as there are image-pull restrictions on DockerHub. DockerHub allows you to pull only 100 images per 6 hour window. Refer to DockerHub rate limiting for more details.</p>"},{"location":"deploy/images/#stable-versions","title":"Stable versions","text":"<p>For stable releases, use version tags (e.g. <code>quay.io/kuberay/operator:v1.0.0</code>).</p>"},{"location":"deploy/images/#master-commits","title":"Master commits","text":"<p>The first seven characters of the git SHA specify images built from specific commits (e.g. <code>quay.io/kuberay/operator:4892ac1</code>).</p>"},{"location":"deploy/images/#nightly-images","title":"Nightly images","text":"<p>The nightly tag specifies images built from the most recent master (e.g. <code>quay.io/kuberay/operator:nightly</code>).</p>"},{"location":"deploy/installation/","title":"YAML","text":""},{"location":"deploy/installation/#installation","title":"Installation","text":"<p>Make sure your Kubernetes cluster and Kubectl are both at version at least 1.23.</p>"},{"location":"deploy/installation/#nightly-version","title":"Nightly version","text":"<pre><code>export KUBERAY_VERSION=master\n\n# Install CRD and KubeRay operator\nkubectl create -k \"github.com/ray-project/kuberay/ray-operator/config/default?ref=${KUBERAY_VERSION}&amp;timeout=90s\"\n</code></pre>"},{"location":"deploy/installation/#stable-version","title":"Stable version","text":""},{"location":"deploy/installation/#method-1-install-charts-from-helm-repository-recommended","title":"Method 1: Install charts from Helm repository (Recommended)","text":"<pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\n\n# Install both CRDs and KubeRay operator\nhelm install kuberay-operator kuberay/kuberay-operator\n</code></pre>"},{"location":"deploy/installation/#method-2-kustomize","title":"Method 2: Kustomize","text":"<pre><code># Install CRD and KubeRay operator\nkubectl create -k \"github.com/ray-project/kuberay/ray-operator/config/default?ref=v1.0.0&amp;timeout=90s\"\n</code></pre>"},{"location":"design/protobuf-grpc-service/","title":"Support proto Core API and RESTful backend services","text":""},{"location":"design/protobuf-grpc-service/#motivation","title":"Motivation","text":"<p>There're few major blockers for users to use KubeRay Operator directly.</p> <ul> <li> <p>Current ray operator is only friendly to users who is familiar with Kubernetes operator pattern. For most data scientists, there's still a learning curve.</p> </li> <li> <p>Using kubectl requires sophisticated permission system. Some kubernetes clusters do not enable user level authentication. In some companies, devops use loose RBAC management and corp SSO system is not integrated with Kubernetes OIDC at all.</p> </li> </ul> <p>For the above reasons, it's worth it to build a generic abstraction on top of the RayCluster CRD. With the core API support, we can easily build backend services, cli, etc to bridge users without Kubernetes experience to KubeRay.</p>"},{"location":"design/protobuf-grpc-service/#goals","title":"Goals","text":"<ul> <li>The API definition should be flexible enough to support different kinds of clients (e.g. backend, cli etc).</li> <li>This backend service underneath should leverage generate clients to interact with existing RayCluster custom resources.</li> <li>New added components should be plugable to existing operator.</li> </ul>"},{"location":"design/protobuf-grpc-service/#proposal","title":"Proposal","text":""},{"location":"design/protobuf-grpc-service/#deployment-topology-and-interactive-flow","title":"Deployment topology and interactive flow","text":"<p>The new gRPC service would be a individual deployment of the KubeRay control plane and user can choose to install it optionally. It will create a service and exposes endpoint to users.</p> <pre><code>NAME                                                      READY   STATUS    RESTARTS      AGE\nkuberay-grpc-service-c8db9dc65-d4w5r                      1/1     Running   0             2d15h\nkuberay-operator-785476b948-fmlm7                         1/1     Running   0             3d\n</code></pre> <p>In issue #29, <code>RayCluster</code> CRD clientset has been generated and gRPC service can leverage it to operate Custom Resources.</p> <p>A simple flow would be like this. (Thanks @akanso for providing the flow) <pre><code>client --&gt; GRPC Server --&gt; [created Custom Resources] &lt;-- Ray Operator (reads CR and accordingly performs CRUD)\n</code></pre></p>"},{"location":"design/protobuf-grpc-service/#api-abstraction","title":"API abstraction","text":"<p>Protocol Buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data. Protoc also provides different community plugins to meet different needs.</p> <p>In order to better define resources at the API level, a few proto files will be defined. Technically, we can use similar data structure like <code>RayCluster</code> Kubernetes resource but this is probably not a good idea.</p> <ul> <li>Some of the Kubernetes API like <code>tolerance</code> and <code>node affinity</code> are too complicated to be converted to an API.</li> <li>We want to leave some flexibility to use database to store history data in the near future (for example, pagination, list options etc).</li> </ul> <p>To resolve these issues, we provide a simple API which can cover most common use-cases. </p> <p>For example, the protobuf definition of the <code>RayCluster</code>:</p> <pre><code>service ClusterService {\n// Creates a new Cluster.\nrpc CreateCluster(CreateClusterRequest) returns (Cluster) {\noption (google.api.http) = {\npost: \"/apis/v1alpha2/namespaces/{namespace}/clusters\"\nbody: \"cluster\"\n};\n}\n\n// Finds a specific Cluster by ID.\nrpc GetCluster(GetClusterRequest) returns (Cluster) {\noption (google.api.http) = {\nget: \"/apis/v1alpha2/namespaces/{namespace}/clusters/{name}\"\n};\n}\n\n// Finds all Clusters in a given namespace. Supports pagination, and sorting on certain fields.\nrpc ListCluster(ListClustersRequest) returns (ListClustersResponse) {\noption (google.api.http) = {\nget: \"/apis/v1alpha2/namespaces/{namespace}/clusters\"\n};\n}\n\n// Finds all Clusters in all namespaces. Supports pagination, and sorting on certain fields.\nrpc ListAllClusters(ListAllClustersRequest) returns (ListAllClustersResponse) {\noption (google.api.http) = {\nget: \"/apis/v1alpha2/clusters\"\n};\n}\n\n// Deletes an cluster without deleting the cluster's runs and jobs. To\n// avoid unexpected behaviors, delete an cluster's runs and jobs before\n// deleting the cluster.\nrpc DeleteCluster(DeleteClusterRequest) returns (google.protobuf.Empty) {\noption (google.api.http) = {\ndelete: \"/apis/v1alpha2/namespaces/{namespace}/clusters/{name}\"\n};\n}\n}\n\nmessage CreateClusterRequest {\n// The cluster to be created.\nCluster cluster = 1;\n// The namespace of the cluster to be created. \nstring namespace = 2;\n}\n\nmessage GetClusterRequest {\n// The name of the cluster to be retrieved.\nstring name = 1;\n// The namespace of the cluster to be retrieved.\nstring namespace = 2;\n}\n\nmessage ListClustersRequest {\n// The namespace of the clusters to be retrieved.\nstring namespace = 1;\n\n}\n\nmessage ListClustersResponse {\n// A list of clusters returned.\nrepeated Cluster clusters = 1;\n}\n\nmessage ListAllClustersRequest {}\n\nmessage ListAllClustersResponse {\n// A list of clusters returned.\nrepeated Cluster clusters = 1;\n}\n\nmessage DeleteClusterRequest {\n// The name of the cluster to be deleted.\nstring name = 1;\n// The namespace of the cluster to be deleted.\nstring namespace = 2;\n}\n\nmessage Cluster {\n// Required input field. Unique cluster name provided by user.\nstring name = 1;\n\n// Required input field. Cluster's namespace provided by user\nstring namespace = 2;\n\n// Required field. This field indicates the user who owns the cluster.\nstring user = 3;\n\n// Optional input field. Ray cluster version\nstring version = 4;\n\n// Optional field.\nenum Environment {\nDEV = 0;\nTESTING = 1;\nSTAGING = 2;\nPRODUCTION = 3;\n}\nEnvironment environment = 5;\n\n// Required field. This field indicates ray cluster configuration\nClusterSpec cluster_spec = 6;\n\n// Output. The time that the cluster created.\ngoogle.protobuf.Timestamp created_at = 7;\n\n// Output. The time that the cluster deleted.\ngoogle.protobuf.Timestamp deleted_at = 8;\n\n// Output. The status to show the cluster status.state\nstring cluster_state = 9;\n\n// Output. The list related to the cluster.\nrepeated ClusterEvent events = 10;\n\n// Output. The service endpoint of the cluster\nmap&lt;string, string&gt; service_endpoint = 11;\n\n// Optional input field. Container environment variables from user.\nmap&lt;string, string&gt; envs = 12;\n}\n\nmessage ClusterSpec {\n// The head group configuration\nHeadGroupSpec head_group_spec = 1;\n// The worker group configurations\nrepeated WorkerGroupSpec worker_group_spec = 2;\n}\n\nmessage Volume {\nstring mount_path = 1;\nenum VolumeType {\nPERSISTENT_VOLUME_CLAIM = 0;\nHOST_PATH = 1;\n}\nVolumeType volume_type = 2;\nstring name = 3;\nstring source = 4;\nbool read_only = 5;\n\n// If indicate hostpath, we need to let user indicate which type \n// they would like to use.\nenum HostPathType {\nDIRECTORY = 0;\nFILE = 1;\n}\nHostPathType host_path_type = 6;\n\nenum MountPropagationMode {\nNONE = 0;\nHOSTTOCONTAINER = 1;\nBIDIRECTIONAL = 2;\n}\nMountPropagationMode mount_propagation_mode = 7;\n}\n\nmessage HeadGroupSpec {\n// Optional. The computeTemplate of head node group\nstring compute_template = 1;\n// Optional field. This field will be used to retrieve right ray container\nstring image = 2;\n// Optional. The service type (ClusterIP, NodePort, Load balancer) of the head node\nstring service_type = 3;\n// Optional. The ray start params of head node group\nmap&lt;string, string&gt; ray_start_params = 4;\n// Optional. The volumes mount to head pod\nrepeated Volume volumes = 5;\n}\n\nmessage WorkerGroupSpec {\n// Required. Group name of the current worker group\nstring group_name = 1;\n// Optional. The computeTemplate of head node group\nstring compute_template = 2;\n// Optional field. This field will be used to retrieve right ray container\nstring image = 3;\n// Required. Desired replicas of the worker group \nint32 replicas = 4;\n// Optional. Min replicas of the worker group \nint32 min_replicas = 5;\n// Optional. Max replicas of the worker group \nint32 max_replicas = 6;\n// Optional. The ray start parames of worker node group\nmap&lt;string, string&gt; ray_start_params = 7;\n// Optional. The volumes mount to worker pods\nrepeated Volume volumes = 8;\n}\n\nmessage ClusterEvent {\n// Output. Unique Event Id.\nstring id = 1;\n\n// Output. Human readable name for event.\nstring name = 2;\n\n// Output. The creation time of the event. \ngoogle.protobuf.Timestamp created_at = 3;\n\n// Output. The last time the event occur.\ngoogle.protobuf.Timestamp first_timestamp = 4;\n\n// Output. The first time the event occur\ngoogle.protobuf.Timestamp last_timestamp = 5;\n\n// Output. The reason for the transition into the object's current status.\nstring reason = 6;\n\n// Output. A human-readable description of the status of this operation.\nstring message = 7;\n\n// Output. Type of this event (Normal, Warning), new types could be added in the future\nstring type = 8;\n\n// Output. The number of times this event has occurred.\nint32 count = 9;\n}\n</code></pre>"},{"location":"design/protobuf-grpc-service/#support-multiple-clients","title":"Support multiple clients","text":"<p>Since we may have different clients to interactive with our services, we will generate gateway RESTful APIs and OpenAPI Spec at the same time.</p> <p></p> <p><code>.proto</code> define core api, grpc and gateway services. go_client and swagger can be generated easily for further usage.</p>"},{"location":"design/protobuf-grpc-service/#grpc-services","title":"gRPC services","text":"<p>The GRPC protocol provides an extremely efficient way of cross-service communication for distributed applications. The public toolkit includes instruments to generate client and server code-bases for many languages allowing the developer to use the most optimal language for the task.</p> <p>The service will implement gPRC server as following graph shows.</p> <ul> <li>A <code>ResourceManager</code> will be used to abstract the implementation of CRUD operators.</li> <li>ClientManager manages kubernetes clients which can operate Kubernetes native resource and custom resources like RayCluster.</li> <li><code>RayClusterClient</code> comes from code generator of CRD. issue#29</li> </ul> <p></p>"},{"location":"design/protobuf-grpc-service/#implementation-history","title":"Implementation History","text":"<ul> <li>2021-11-25: inital proposal accepted.</li> <li>2022-12-01: new protobuf definition released.</li> </ul> <p>Note: we should update doc when there's a large update.</p>"},{"location":"development/development/","title":"KubeRay Development Guide","text":"<p>This guide provides an overview of the different components in the KubeRay project and instructions for developing and testing each component. Most developers will be concerned with the KubeRay Operator; the other components are optional.</p>"},{"location":"development/development/#kuberay-operator","title":"KubeRay Operator","text":"<p>The KubeRay Operator is responsible for managing Ray clusters on Kubernetes. To learn more about developing and testing the KubeRay Operator, please refer to the Operator Development Guide.</p>"},{"location":"development/development/#kuberay-apiserver","title":"KubeRay APIServer","text":"<p>The KubeRay APIServer is a central component that exposes the KubeRay API for managing Ray clusters. For more information about developing and testing the KubeRay APIServer, please refer to the APIServer Development Guide.</p>"},{"location":"development/development/#kuberay-cli","title":"KubeRay CLI","text":"<p>The KubeRay CLI is a command-line interface for interacting with Ray clusters managed by KubeRay. For more information about developing and testing the KubeRay CLI, please refer to the CLI Development Guide.</p>"},{"location":"development/development/#proto-and-openapi","title":"Proto and OpenAPI","text":"<p>KubeRay uses Protocol Buffers (protobuf) and OpenAPI specifications to define the API and data structures. For more information about developing and testing proto files and OpenAPI specifications, please refer to the Proto and OpenAPI Development Guide.</p>"},{"location":"development/development/#deploying-documentation-locally","title":"Deploying Documentation Locally","text":"<p>To preview the KubeRay documentation locally, follow these steps:</p> <ul> <li>Make sure you have Docker installed on your machine.</li> <li>Open a terminal and navigate to the root directory of your KubeRay repository.</li> <li>Run the following command:</li> </ul> <pre><code>docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material\n</code></pre> <ul> <li>Open your web browser and navigate to http://0.0.0.0:8000/kuberay/ to view the documentation.</li> </ul> <p>If you make any changes to the documentation files, the local preview will automatically update to reflect those changes.</p>"},{"location":"development/release/","title":"KubeRay Release Process","text":""},{"location":"development/release/#prerequisite","title":"Prerequisite","text":"<p>You need KubeRay GitHub write permissions to cut a release branch and create a release tag.</p>"},{"location":"development/release/#overview","title":"Overview","text":"<p>Each major release (e.g. <code>0.4</code>) is managed in its own GitHub branch. To release KubeRay, cut a release branch (e.g. <code>release-0.4</code>) from master and build commits on that branch until you reach a satisfactory final release commit.</p> <p>Immediately after cutting the release branch, create a commit for a release candidate (e.g. <code>0.4.0-rc.0</code>), and build the associated artifacts (images and charts). If further changes need to be made to the release, pick changes from the master branch into the release branch. Make as many release candidates as necessary until a stable final release commit is reached. Then build final release artifacts, publish release notes, and announce the release.</p>"},{"location":"development/release/#kuberay-release-schedule","title":"KubeRay release schedule","text":"<p>KubeRay release plans to synchronize with every two Ray releases. KubeRay v0.5.0 synchronizes with Ray 2.4.0, so v0.6.0 should synchronize with Ray 2.6.0.</p> <ul> <li>KubeRay feature freeze: Two weeks before the official Ray release.</li> <li>KubeRay release: One week before the official Ray release.</li> <li>Update KubeRay documentation in Ray repository: Finish before the official Ray release.</li> </ul>"},{"location":"development/release/#steps","title":"Steps","text":""},{"location":"development/release/#step-0-kuberay-feature-freeze","title":"Step 0. KubeRay feature freeze","text":"<p>Ensure the last master commit you want to release passes the Go-build-and-test workflow before feature freeze.</p>"},{"location":"development/release/#step-1-ensure-that-the-desired-master-commit-is-stable","title":"Step 1. Ensure that the desired master commit is stable","text":"<p>Ensure that the desired master commit is stable by verifying the following:</p> <ul> <li>The KubeRay documentation is up-to-date.</li> <li>All example configurations use the latest released version of Ray.</li> <li>The example configurations work.</li> </ul> <p>During the KubeRay <code>0.5.0</code> release, we used spreadsheets to track manual testing and documentation updates. Instead of using the latest stable release of KubeRay (i.e., v0.4.0 for the v0.5.0 release process), we should verify the master branch using the following:</p> <ul> <li>The nightly KubeRay operator Docker image: <code>kuberay/operator:nightly</code>.</li> <li>The local CRD / YAML / Helm charts.</li> </ul> <p>Open PRs to track the progress of manual testing for documentation, but avoid merging these PRs until the  Docker images and Helm charts for v0.5.0 are available  (example PRs: #997, #999, #1004, #1012). Bug fix pull requests to fix bugs which found in the documentation testing process can be merged (example PR: #1000).</p> <p>Manual testing can be time-consuming, and to relieve the workload, we plan to add more CI tests. The minimum requirements to move forward are:</p> <ul> <li>All example configurations can work with <code>kuberay/operator:nightly</code> and the latest release of Ray (i.e. 2.3.0 for KubeRay v0.5.0).</li> <li>Update all version strings in the documents.</li> </ul>"},{"location":"development/release/#step-2-create-a-new-branch-in-ray-projectkuberay-repository","title":"Step 2. Create a new branch in ray-project/kuberay repository","text":"<ul> <li>Depending on whether the release is for a major, minor, or patch version, take the following steps.</li> <li>Major or Minor version (e.g. <code>0.5.0</code> or <code>1.0.0</code>). Create a release branch named <code>release-X.Y</code>:     <pre><code>git checkout -b release-0.5\ngit push -u upstream release-0.5\n</code></pre></li> <li>Patch version (e.g. <code>0.5.1</code>). You don't need to cut a release branch for a patch version. Instead add commits to the release branch.</li> </ul>"},{"location":"development/release/#step-3-create-a-first-release-candidate-v050-rc0","title":"Step 3. Create a first release candidate (<code>v0.5.0-rc.0</code>).","text":"<ul> <li> <p>Merge a PR into the release branch updating Helm chart versions, Helm chart image tags, and kustomize manifest image tags. For <code>v0.5.0-rc0</code>, we did this in PR #1001</p> </li> <li> <p>Release <code>rc0</code> images using the release-image-build workflow on GitHub actions. You will be prompted for a commit reference and an image tag. The commit reference should be the SHA of the tip of the release branch. The image tag should be <code>vX.Y.Z-rc.0</code>.</p> </li> <li> <p>Tag the tip of release branch with <code>vX.Y.Z-rc.0</code>.     <pre><code>git tag v0.5.0-rc.0\ngit push upstream v0.5.0-rc.0\n</code></pre></p> </li> <li> <p>The image release CI pipeline also publishes the <code>github.com/ray-project/kuberay/ray-operator@vX.Y.Z-rc.0</code> Go module. KubeRay has supported Go modules since v0.6.0. Follow these instructions to verify the Go module installation.     <pre><code># Install the module. This step is highly possible to fail because the module is not available in the proxy server.\ngo install github.com/ray-project/kuberay/ray-operator@v1.0.0-rc.0\n\n# Make the module available by running the go list command to prompt Go to update its index of modules with information about the module you\u2019re publishing.\n# See https://go.dev/doc/modules/publishing for more details.\nGOPROXY=proxy.golang.org go list -m github.com/ray-project/kuberay/ray-operator@v1.0.0-rc.0\n# [Expected output]: github.com/ray-project/kuberay/ray-operator v1.0.0-rc.0\n\n# Wait for a while until the URL https://sum.golang.org/lookup/github.com/ray-project/kuberay/ray-operator@vX.Y.Z-rc.0 no longer displays \"not found\". This may take 15 mins based on my experience.\ngo install github.com/ray-project/kuberay/ray-operator@v1.0.0-rc.0\n\n# Check the module is installed successfully.\nls $GOPATH/pkg/mod/github.com/ray-project/kuberay/\n# [Expected output]: ray-operator@v1.0.0-rc.0\n</code></pre></p> </li> <li> <p>Release rc0 Helm charts following the instructions.</p> </li> <li> <p>Open a PR into the Ray repo updating the operator version used in the autoscaler integration test. Make any adjustments necessary for the test to pass (example). Make sure the test labelled kubernetes-operator passes before merging.</p> </li> <li> <p>Announce the <code>rc0</code> release on the KubeRay slack, with deployment instructions (example).</p> </li> </ul>"},{"location":"development/release/#step-4-create-more-release-candidates-rc1-rc2-if-necessary","title":"Step 4. Create more release candidates (<code>rc1</code>, <code>rc2</code>, ...) if necessary","text":"<ul> <li>Resolve issues with the release branch by cherry-picking master commits into the release branch.</li> <li>When cherry-picking changes, it is best to open a PR against the release branch -- don't push directly to the release branch.</li> <li>Follow step 3 to create new Docker images and Helm charts for the new release candidate.</li> </ul>"},{"location":"development/release/#step-5-create-a-final-release","title":"Step 5. Create a final release","text":"<ul> <li>Create a final release (i.e. v0.5.0) by repeating step 4 once more using the tag of the release (<code>vX.Y.Z</code>) with no <code>-rc</code> suffix.</li> </ul>"},{"location":"development/release/#step-6-merge-open-prs-in-step-1-and-post-release-prs","title":"Step 6. Merge open PRs in step 1 and post-release PRs","text":"<p>Now, we have the Docker images and Helm charts for v0.5.0. </p> <ul> <li> <p>Merge the pull requests in Step 1 (i.e. #997, #999, #1004, #1012)</p> </li> <li> <p>Merge post-release pull requests (example: #1010). See here to understand the definition of \"post-release\" and the compatibility philosophy for KubeRay.</p> </li> </ul>"},{"location":"development/release/#step-7-update-kuberay-documentation-in-ray-repository","title":"Step 7. Update KubeRay documentation in Ray repository.","text":"<ul> <li>Update KubeRay documentation in Ray repository with v0.5.0. Examples for v0.5.0:<ul> <li>https://github.com/ray-project/ray/pull/33339</li> <li>https://github.com/ray-project/ray/pull/34178</li> </ul> </li> </ul>"},{"location":"development/release/#step-8-generate-release","title":"Step 8. Generate release","text":"<ul> <li>Click \"Create release\" to create release for the tag v0.5.0 (link). </li> <li>Run <code>make release</code> in cli folder and generate <code>kuberay-$VERSION-darwin-amd64.zip</code> and <code>kuberay-$VERSION-linux-amd64.zip</code> files. Upload them to the GitHub release.</li> <li>Follow the instructions to generate release notes and add notes in the GitHub release.</li> </ul>"},{"location":"development/release/#step-9-announce-the-release-on-the-kuberay-slack","title":"Step 9. Announce the release on the KubeRay slack!","text":"<ul> <li>Announce the release on the KubeRay slack (example)!</li> </ul>"},{"location":"development/release/#step-10-update-changelog","title":"Step 10. Update CHANGELOG","text":"<ul> <li>Send a PR to add the release notes to CHANGELOG.md.</li> </ul>"},{"location":"development/release/#step-11-update-and-improve-this-release-document","title":"Step 11. Update and improve this release document!","text":"<ul> <li>Update this document and optimize the release process!</li> </ul>"},{"location":"guidance/FAQ/","title":"FAQ","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/autoscaler/","title":"Autoscaling","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/aws-eks-gpu-cluster/","title":"Aws eks gpu cluster","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/aws-eks-iam/","title":"IAM roles for service account on AWS EKS","text":"<p>Applications in a pod's containers can use an AWS SDK or the AWS CLI to make API requests to AWS services using AWS Identity and Access Management (IAM) permissions. Applications must sign their AWS API requests with AWS credentials. IAM roles for service accounts provide the ability to manage credentials for your applications. To achieve this, you can read the following articles:</p> <ul> <li>IAM roles for service accounts: This is the official AWS documentation that explains IAM roles for service accounts step-by-step.</li> <li>Understanding IAM roles for service accounts, IRSA, on AWS EKS.: Good article to provide an easy-to-understand explanation.</li> </ul>"},{"location":"guidance/aws-eks-iam/#pitfall","title":"Pitfall","text":"<ul> <li>It's worth noting that this pitfall occurs in Ray images prior to version 2.5.0.</li> </ul> <p>For example, users may want to download their files from their S3 bucket with AWS Python SDK (<code>boto3</code>) in Ray Pods. However, there is a pitfall in the Ray images. When you execute the boto3_example_1.py in a Ray Pod, you will get an error like <code>An error occurred (403) when calling the HeadObject operation: Forbidden</code> even if your pod is attached to a service account which has an IAM role that is able to access the S3 bucket.</p> <pre><code># boto3_example_1.py\nimport boto3\n\ns3 = boto3.client('s3')\nbucket_name = YOUR_BUCKET_NAME\nkey = YOUR_OBJECT_KEY\nfilename = YOUR_FILENAME\n\ns3.download_file(bucket_name, key, filename)\n</code></pre> <p>The root cause is that the version of <code>boto3</code> in the Ray image is too old. To elaborate, <code>rayproject/ray:2.3.0</code> provides boto3 version 1.4.8 (Nov. 21, 2017), a more recent version (1.26) is currently available as per https://pypi.org/project/boto3/#history. The <code>boto3</code> 1.4.8 does not support to initialize the security credentials automatically in some cases (e.g. <code>AssumeRoleWithWebIdentity</code>).</p> <pre><code># image: rayproject/ray:2.5.0\npip freeze | grep boto\n# boto3==1.4.8\n# botocore==1.8.50\n</code></pre> <p>Another issue that users may encounter is related to RayService. If the <code>working_dir</code> for RayService is set to a zip file located in a private S3 bucket, it can prevent the Ray Serve application from starting. Users can confirm this by executing <code>serve status</code> in the head Pod, which will return an error like <code>An error occurred (AccessDenied) when calling the GetObject operation: Access Denied</code>. In this case, users can build their custom images with upgrading the <code>boto3</code> package (i.e. Solution 2).</p>"},{"location":"guidance/aws-eks-iam/#workaround-solutions","title":"Workaround solutions","text":""},{"location":"guidance/aws-eks-iam/#solution-1-setup-the-credentials-explicitly","title":"Solution 1: Setup the credentials explicitly","text":"<pre><code># boto3_example_2.py\nimport os\nimport boto3\ndef assumed_role_session():\n    role_arn = os.getenv('AWS_ROLE_ARN')\n    with open(os.getenv(\"AWS_WEB_IDENTITY_TOKEN_FILE\"), 'r') as content_file:\n        web_identity_token = content_file.read()\n    role = boto3.client('sts').assume_role_with_web_identity(RoleArn=role_arn, RoleSessionName='assume-role',\n                                                             WebIdentityToken=web_identity_token)\n    credentials = role['Credentials']\n    aws_access_key_id = credentials['AccessKeyId']\n    aws_secret_access_key = credentials['SecretAccessKey']\n    aws_session_token = credentials['SessionToken']\n    return boto3.session.Session(aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key,\n                              aws_session_token=aws_session_token)\n\nsession = assumed_role_session()\ns3 = session.client(\"s3\")\nbucket_name = YOUR_BUCKET_NAME\nkey = YOUR_OBJECT_KEY\nfilename = YOUR_FILENAME\ns3.download_file(bucket_name, key, filename)\n</code></pre>"},{"location":"guidance/aws-eks-iam/#solution-2-upgrade-the-boto3-package","title":"Solution 2: Upgrade the boto3 package","text":"<pre><code>pip install --upgrade boto3\npython3 -m pip install -U pyOpenSSL cryptography\npython3 boto3_example_1.py # success\n</code></pre>"},{"location":"guidance/gcp-gke-gpu-cluster/","title":"Gcp gke gpu cluster","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/gcs-ft/","title":"Ray GCS Fault Tolerance","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/ingress/","title":"Ingress","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/kubeflow-integration/","title":"Kubeflow Integration","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/kuberay-with-MCAD/","title":"KubeRay integration with MCAD (Multi-Cluster-App-Dispatcher)","text":"<p>The multi-cluster-app-dispatcher is a Kubernetes controller providing mechanisms for applications to manage batch jobs in a single or multi-cluster environment. For more details please refer here.</p>"},{"location":"guidance/kuberay-with-MCAD/#use-case","title":"Use case","text":"<p>MCAD allows you to deploy Ray cluster with a guarantee that sufficient resources are available in the cluster prior to actual pod creation in the Kubernetes cluster. It supports features such as:</p> <ul> <li>Integrates with upstream Kubernetes scheduling stack for features such co-scheduling, Packing on GPU dimension etc.</li> <li>Ability to wrap any Kubernetes objects.</li> <li>Increases control plane stability by JIT (Just-in Time) object creation.</li> <li>Queuing with policies.</li> <li>Quota management that goes across namespaces.</li> <li>Support for multiple Kubernetes clusters; dispatching jobs to any one of a number of Kubernetes clusters.</li> </ul> <p>In order to queue Ray cluster(s) and <code>gang dispatch</code> them when aggregated resources are available please create a KinD cluster using the instruction below and then refer to the setup KubeRay-MCAD integration on a Kubernetes Cluster or an OpenShift Cluster.</p> <p>On OpenShift, MCAD and KubeRay are already part of the Open Data Hub Distributed Workload Stack. The stack provides a simple, user-friendly abstraction for scaling, queuing and resource management of distributed AI/ML and Python workloads. Please follow the Quick Start in the Distributed Workloads for installation.</p>"},{"location":"guidance/kuberay-with-MCAD/#create-kind-cluster","title":"Create KinD cluster","text":"<p>We need a KinD cluster with the specified cluster resources to consistently observe the expected behavior described in the demo below. This can be done with running KinD with Podman.</p> <p>Note: Without Podman, a KinD worker node is allowed to see the cpu/memory resources on the host. In addition, this environment is created to run the tutorial on a resource-constrained local Kubernetes environment. It is not recommended for real workloads or production. <pre><code>podman machine init --cpus 8 --memory 8196\npodman machine start\npodman machine list\n</code></pre> Expect the Podman Machine running with the follow CPU and MEMORY resources <pre><code>NAME                     VM TYPE     CREATED        LAST UP            CPUS        MEMORY      DISK SIZE\npodman-machine-default*  qemu        2 minutes ago  Currently running  8           8.594GB     107.4GB\n</code></pre> Create KinD cluster on the Podman Machine: <pre><code>KIND_EXPERIMENTAL_PROVIDER=podman kind create cluster\n</code></pre> Creating a KinD cluster should take less than 1 minute. Expect the output similar to: <pre><code>using podman due to KIND_EXPERIMENTAL_PROVIDER\nenabling experimental podman provider\nCreating cluster \"kind\" ...\n \u2713 Ensuring node image (kindest/node:v1.26.3) \ud83d\uddbc\n \u2713 Preparing nodes \ud83d\udce6\n \u2713 Writing configuration \ud83d\udcdc\n \u2713 Starting control-plane \ud83d\udd79\ufe0f\n \u2713 Installing CNI \ud83d\udd0c\n \u2713 Installing StorageClass \ud83d\udcbe\nSet kubectl context to \"kind-kind\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-kind\n\nHave a nice day! \ud83d\udc4b\n</code></pre></p> <p>Describe the single node cluster: <pre><code>kubectl describe node kind-control-plane\n</code></pre></p> <p>Expect the <code>cpu</code> and <code>memory</code> in the <code>Allocatable</code> section to be similar to: <pre><code>Allocatable:\n  cpu:            8\n  hugepages-1Gi:  0\n  hugepages-2Mi:  0\n  memory:         8118372Ki\n  pods:           110\n</code></pre></p>"},{"location":"guidance/kuberay-with-MCAD/#submitting-kuberay-cluster-to-mcad","title":"Submitting KubeRay cluster to MCAD","text":"<p>After the KinD cluster is created using the instruction above, make sure to install the KubeRay-MCAD integration Prerequisites for KinD cluster.</p> <p>Let's create two RayClusters using the AppWrapper custom resource(CR) on the same Kubernetes cluster. The AppWrapper is the custom resource definition provided by MCAD to dispatch resources and manage batch jobs on Kubernetes clusters.</p> <ul> <li>We submit the first RayCluster with the AppWrapper CR aw-raycluster.yaml:</li> </ul> <p><pre><code>kubectl create -f https://raw.githubusercontent.com/project-codeflare/multi-cluster-app-dispatcher/main/doc/usage/examples/kuberay/config/aw-raycluster.yaml\n</code></pre>   In the above AppWrapper CR, we wrapped an example of RayCluster CR in the <code>generictemplate</code>. We also specified matching resources for each of the RayCluster Head node and worker node in the <code>custompodresources</code>. The MCAD uses the <code>custompodresources</code> to reserve the required resources to run the RayCluster without creating pending Pods.</p> <p>Note: Within the same AppWrapper, you may also wrap any individual k8s resources (i.e. configMap, secret, etc) associated with this job as a generictemplate to be dispatched together with the RayCluster.</p> <p>Check AppWrapper status by describing the job.   <pre><code>kubectl describe appwrapper raycluster-complete -n default\n</code></pre></p> <p>The <code>Status:</code> stanza would show the <code>State</code> of <code>Running</code> if the wrapped RayCluster has been deployed. The 2 Pods associated with the RayCluster were also created.   <pre><code>Status:\n  Canrun:  true\n  Conditions:\n    Last Transition Micro Time:  2023-08-29T02:50:18.829462Z\n    Last Update Micro Time:      2023-08-29T02:50:18.829462Z\n    Status:                      True\n    Type:                        Init\n    Last Transition Micro Time:  2023-08-29T02:50:18.829496Z\n    Last Update Micro Time:      2023-08-29T02:50:18.829496Z\n    Reason:                      AwaitingHeadOfLine\n    Status:                      True\n    Type:                        Queueing\n    Last Transition Micro Time:  2023-08-29T02:50:18.842010Z\n    Last Update Micro Time:      2023-08-29T02:50:18.842010Z\n    Reason:                      FrontOfQueue.\n    Status:                      True\n    Type:                        HeadOfLine\n    Last Transition Micro Time:  2023-08-29T02:50:18.902379Z\n    Last Update Micro Time:      2023-08-29T02:50:18.902379Z\n    Reason:                      AppWrapperRunnable\n    Status:                      True\n    Type:                        Dispatched\n  Controllerfirsttimestamp:      2023-08-29T02:50:18.829462Z\n  Filterignore:                  true\n  Queuejobstate:                 Dispatched\n  Sender:                        before manageQueueJob - afterEtcdDispatching\n  State:                         Running\nEvents:                          &lt;none&gt;\n(base) asmalvan@mcad-dev:~/mcad-kuberay$ kubectl get pod -n default\nNAME                                           READY   STATUS    RESTARTS   AGE\nraycluster-complete-head-9s4x5                 1/1     Running   0          47s\nraycluster-complete-worker-small-group-4s6jv   1/1     Running   0          47s\n</code></pre></p> <ul> <li>Let's submit another RayCluster with the AppWrapper CR and see it queued without creating pending Pods using the command:   <pre><code>kubectl create -f https://raw.githubusercontent.com/project-codeflare/multi-cluster-app-dispatcher/main/doc/usage/examples/kuberay/config/aw-raycluster-1.yaml\n</code></pre>   Check the raycluster-complete-1 AppWrapper   <pre><code>kubectl describe appwrapper raycluster-complete-1 -n default\n</code></pre>   The <code>Status:</code> stanza should show the <code>State</code> of <code>Pending</code> if the wrapped object (RayCluster) has been queued. No pods from the second <code>AppWrapper</code> were created due to <code>Insufficient resources to dispatch AppWrapper</code>.   <pre><code>Status:\n  Conditions:\n    Last Transition Micro Time:  2023-08-29T17:39:08.406401Z\n    Last Update Micro Time:      2023-08-29T17:39:08.406401Z\n    Status:                      True\n    Type:                        Init\n    Last Transition Micro Time:  2023-08-29T17:39:08.406452Z\n    Last Update Micro Time:      2023-08-29T17:39:08.406451Z\n    Reason:                      AwaitingHeadOfLine\n    Status:                      True\n    Type:                        Queueing\n    Last Transition Micro Time:  2023-08-29T17:39:08.423208Z\n    Last Update Micro Time:      2023-08-29T17:39:08.423208Z\n    Reason:                      FrontOfQueue.\n    Status:                      True\n    Type:                        HeadOfLine\n    Last Transition Micro Time:  2023-08-29T17:39:08.439753Z\n    Last Update Micro Time:      2023-08-29T17:39:08.439753Z\n    Message:                     Insufficient resources to dispatch AppWrapper.\n    Reason:                      AppWrapperNotRunnable.\n    Status:                      True\n    Type:                        Backoff\n  Controllerfirsttimestamp:      2023-08-29T17:39:08.406399Z\n  Filterignore:                  true\n  Queuejobstate:                 Backoff\n  Sender:                        before ScheduleNext - setHOL\n  State:                         Pending\nEvents:                          &lt;none&gt;\n</code></pre></li> </ul> <p>We may manually check the allocated resources: <pre><code>kubectl describe node kind-control-plane\n</code></pre> The <code>Allocated resources</code> section showed cpu Requests as 6050m(75%) therefore the remaining cpu resource did not satisfy the second AppWrapper. <pre><code>Allocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                6050m (75%)      5200m (65%)\n  memory             6824650Ki (84%)  6927050Ki (85%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-1Gi      0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\n</code></pre> Dispatching policy out of the box is FIFO which can be augmented as per user needs. The second RayCluster will be dispatched when additional aggregated resources are available in the cluster or the first AppWrapper is deleted.</p> <p>For example, observe the other RayCluster been created after deleting the first AppWrapper using:</p> <pre><code>kubectl delete appwrapper raycluster-complete -n default\n</code></pre> <p>Note: This would also simultaneously remove any K8s resources you may have wrapped as generictemplates within this AppWrapper.</p>"},{"location":"guidance/mobilenet-rayservice/","title":"Mobilenet rayservice","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/observability/","title":"Observability","text":""},{"location":"guidance/observability/#raycluster-status","title":"RayCluster Status","text":""},{"location":"guidance/observability/#state","title":"State","text":"<p>In the RayCluster resource definition, we use <code>State</code> to represent the current status of the Ray cluster.</p> <p>For now, there are three types of the status exposed by the RayCluster's status.state: <code>ready</code>, <code>unhealthy</code> and <code>failed</code>.</p> State Description ready The Ray cluster is ready for use. unhealthy The <code>rayStartParams</code> are misconfigured and the Ray cluster may not function properly. failed A severe issue has prevented the head node or worker nodes from starting. <p>If you use the apiserver to retrieve the resource, you may find the state in the <code>clusterState</code> field.</p> <pre><code>curl --request GET '&lt;baseUrl&gt;/apis/v1alpha2/namespaces/&lt;namespace&gt;/clusters/&lt;raycluster-name&gt;'\n{\n\"name\": \"&lt;raycluster-name&gt;\",\n\"namespace\": \"&lt;namespace&gt;\",\n//...\n\"createdAt\": \"2022-08-10T10:31:25Z\",\n\"clusterState\": \"ready\",\n//...\n}\n</code></pre>"},{"location":"guidance/observability/#endpoint","title":"Endpoint","text":"<p>If you use the nodeport as service to expose the raycluster endpoint, like dashboard or redis, there are <code>endpoints</code> field in the status to record the service endpoints.</p> <p>you can directly use the ports in the <code>endpoints</code> to connect to the related service.</p> <p>Also, if you use apiserver to retrieve the resource, you can find the endpoints in the <code>serviceEndpoint</code> field.</p> <pre><code>curl --request GET '&lt;baseUrl&gt;/apis/v1alpha2/namespaces/&lt;namespace&gt;/clusters/&lt;raycluster-name&gt;'\n{\n\"name\": \"&lt;raycluster-name&gt;\",\n\"namespace\": \"&lt;namespace&gt;\",\n//...\n\"serviceEndpoint\": {\n\"dashboard\": \"30001\",\n\"head\": \"30002\",\n\"metrics\": \"30003\",\n\"redis\": \"30004\"\n},\n//...\n}\n</code></pre>"},{"location":"guidance/observability/#ray-cluster-monitoring-with-prometheus-grafana","title":"Ray Cluster: Monitoring with Prometheus &amp; Grafana","text":"<p>See prometheus-grafana.md for more details.</p>"},{"location":"guidance/observability/#profiling-with-kuberay","title":"Profiling with KubeRay","text":"<p>See profiling.md for more details.</p>"},{"location":"guidance/pod-command/","title":"Executing Commands","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/pod-security/","title":"Pod Security","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/profiling/","title":"Profiling","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/prometheus-grafana/","title":"Prometheus and Grafana","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/rayStartParams/","title":"rayStartParams","text":""},{"location":"guidance/rayStartParams/#default-ray-start-parameters-for-kuberay","title":"Default Ray Start Parameters for KubeRay","text":"<p>This document outlines the default settings for <code>rayStartParams</code> in KubeRay.</p>"},{"location":"guidance/rayStartParams/#options-exclusive-to-the-head-pod","title":"Options Exclusive to the Head Pod","text":"<ul> <li> <p><code>--dashboard-host</code>: Host for the dashboard server, either <code>localhost</code> (127.0.0.1) or <code>0.0.0.0</code>. The latter setting exposes the Ray dashboard outside the Ray cluster, which is required when ingress is utilized for Ray cluster access. The default value for both Ray and KubeRay 0.5.0 is <code>localhost</code>. Please note that this will change for versions of KubeRay later than 0.5.0, where the default setting will be <code>0.0.0.0</code>.</p> </li> <li> <p><code>--no-monitor</code> (Modification is not recommended):</p> </li> <li>Ray autoscaler supports various node providers such as AWS, GCP, Azure, and Kubernetes. However, the default autoscaler is not compatible with Kubernetes. Therefore, when KubeRay autoscaling is enabled (i.e. <code>EnableInTreeAutoscaling</code> is true), KubeRay disables the monitor process via setting <code>--no-monitor</code> to true and injects a sidecar container for KubeRay autoscaler. See PR #13505 for more details.</li> <li> <p>Please note that the monitor process serves not only for autoscaling but also for observability, such as Prometheus metrics. Considering this, it is reasonable to disable the Kubernetes-incompatible autoscaler regardless of the value of <code>EnableInTreeAutoscaling</code>. To achieve this, we can launch the monitor process without autoscaling functionality by setting the autoscaler to READONLY mode. If <code>autoscaling-option</code> is not set, the autoscaler will default to READONLY mode.</p> </li> <li> <p><code>--port</code>: Port for the GCS server. The port is set to <code>6379</code> by default. Please ensure that this value matches the <code>gcs-server</code> container port in Ray head container.</p> </li> <li> <p><code>--redis-password</code>: Redis password for an external Redis, necessary when fault tolerance is enabled.  The default value is <code>\"\"</code> after Ray 2.3.0. See #929 for more details. </p> </li> </ul>"},{"location":"guidance/rayStartParams/#options-exclusive-to-the-worker-pods","title":"Options Exclusive to the worker Pods","text":"<ul> <li><code>--address</code>: Address of the GCS server. Worker pods utilize this address to establish a connection with the Ray cluster. By default, this address takes the form <code>&lt;FQDN&gt;:&lt;GCS_PORT&gt;</code>. The <code>GCS_PORT</code> corresponds to the value set in the <code>--port</code> option. For more insights on Fully Qualified Domain Name (FQDN), refer to PR #938 and PR #951.</li> </ul>"},{"location":"guidance/rayStartParams/#options-applicable-to-both-head-and-worker-pods","title":"Options Applicable to Both Head and Worker Pods","text":"<ul> <li> <p><code>--block</code>: This option blocks the ray start command indefinitely. It will be automatically set by KubeRay. See PR #675 for more details. Modification is not recommended.</p> </li> <li> <p><code>--memory</code>: Amount of memory on this Ray node. Default is determined by Ray container resource limits. Modify Ray container resource limits instead of this option. See PR #170.</p> </li> <li> <p><code>--metrics-export-port</code>: Port for exposing Ray metrics through a Prometheus endpoint. The port is set to <code>8080</code> by default. Please ensure that this value matches the <code>metrics</code> container port if you need to customize it. See PR #954 and prometheus-grafana doc for more details.</p> </li> <li> <p><code>--num-cpus</code>: Number of logical CPUs on this Ray node. Default is determined by Ray container resource limits. Modify Ray container resource limits instead of this option. See PR #170. However, it is sometimes useful to override this autodetected value. For example, setting <code>num-cpus:\"0\"</code> for the Ray head pod will prevent Ray workloads with non-zero CPU requirements from being scheduled on the head.</p> </li> <li> <p><code>--num-gpus</code>: Number of GPUs on this Ray node. Default is determined by Ray container resource limits. Modify Ray container resource limits instead of this option. See PR #170.</p> </li> </ul>"},{"location":"guidance/rayclient-nginx-ingress/","title":"Connect to Rayclient with NGINX Ingress","text":"<p>Warning: Ray client has some known limitations and is not actively maintained.</p> <p>This document provides an example for connecting Ray client to a Raycluster via NGINX Ingress on Kind. Although this is a Kind example, the steps applies to any Kubernetes Cluster that runs the NGINX Ingress Controller.</p>"},{"location":"guidance/rayclient-nginx-ingress/#requirements","title":"Requirements","text":"<ul> <li> <p>Environment:</p> <ul> <li><code>Ubuntu</code></li> <li><code>Kind</code></li> </ul> </li> <li> <p>Computing resources:</p> <ul> <li>16GB RAM</li> <li>8 CPUs</li> </ul> </li> </ul>"},{"location":"guidance/rayclient-nginx-ingress/#step-1-create-a-kind-cluster","title":"Step 1: Create a Kind cluster","text":"<p>The extra arg prepares the Kind cluster for deploying the ingress controller <pre><code>cat &lt;&lt;EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 80\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 443\n    protocol: TCP\nEOF\n</code></pre></p>"},{"location":"guidance/rayclient-nginx-ingress/#step-2-deploy-nginx-ingress-controller","title":"Step 2: Deploy NGINX Ingress Controller","text":"<p>The SSL Passthrough feature is required to pass on the encryption to the backend service directly. <pre><code># Deploy the NGINX Ingress Controller\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\n\n# Turn on SSL Passthrough\nkubectl patch deploy --type json --patch '[{\"op\":\"add\",\"path\": \"/spec/template/spec/containers/0/args/-\",\"value\":\"--enable-ssl-passthrough\"}]' ingress-nginx-controller -n ingress-nginx\n\n# Verify log has Starting TLS proxy for SSL Passthrough\nkubectl logs deploy/ingress-nginx-controller -n ingress-nginx\n</code></pre></p>"},{"location":"guidance/rayclient-nginx-ingress/#step-3-install-kuberay-operator","title":"Step 3: Install KubeRay operator","text":"<p>Follow this document to install the latest stable KubeRay operator via Helm repository.</p>"},{"location":"guidance/rayclient-nginx-ingress/#step-4-create-a-raycluster-with-tls-enabled","title":"Step 4: Create a Raycluster with TLS enabled","text":"<p>The Ray client server is a GRPC service. The NGINX Ingress Controller supports GRPC backend service which uses http/2 and requires secured connection. The command below creates a Raycluster with TLS enabled: <pre><code>kubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-cluster.tls.yaml\n</code></pre> Refer to the TLS document for more detail.</p>"},{"location":"guidance/rayclient-nginx-ingress/#step-5-create-an-ingress-for-the-ray-client-service","title":"Step 5: Create an ingress for the Ray client service","text":"<p>With the Raycluster running, create an ingress for the Ray client backend service using the rayclient-ingress example below: <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rayclient-ingress\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\n    nginx.ingress.kubernetes.io/backend-protocol: \"GRPC\"\nspec:\n  rules:\n    - host: \"localhost\"\n      http:\n        paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: raycluster-tls-head-svc\n              port:\n                number: 10001\nEOF\n</code></pre> The annotation, <code>nginx.ingress.kubernetes.io/backend-protocol: \"GRPC\"</code> sets up the appropriate NGINX configuration to route http/2 traffic to a GRPC backend service. The <code>nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"</code> annotation tells the ingress to forward the encrypted traffic to the backend service to be handled inside the Raycluster itself.</p>"},{"location":"guidance/rayclient-nginx-ingress/#step-6-connecting-to-ray-client-service-via-the-ingress","title":"Step 6: Connecting to Ray client service via the ingress","text":"<p>Since the Raycluster uses TLS, the local Ray client would require a set of certificates to connect to Raycluster.</p> <p>Warning: Ray client has some known limitations and is not actively maintained. <pre><code># Download the ca key pair and create a cert signing request (CSR)\nkubectl get secret ca-tls -o template='{{index .data \"ca.key\"}}'|base64 -d &gt; ./ca.key\nkubectl get secret ca-tls -o template='{{index .data \"ca.crt\"}}'|base64 -d &gt; ./ca.crt\nopenssl req -nodes -newkey rsa:2048 -keyout ./tls.key -out ./tls.csr -subj '/CN=local'\ncat &lt;&lt;EOF &gt;./cert.conf\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints=CA:FALSE\nsubjectAltName = @alt_names\n[alt_names]\nDNS.1 = localhost\nIP.1 = 127.0.0.1\nEOF\n# Sign and create a tls cert\nopenssl x509 -req -CA ./ca.crt -CAkey ./ca.key -in ./tls.csr -out ./tls.crt -days 365 -CAcreateserial -extfile ./cert.conf\n\n# Connect Ray client to the Raycluster using the tls keypair and the ca cert\npython -c '\nimport os\nimport ray\nos.environ[\"RAY_USE_TLS\"] = \"1\"\nos.environ[\"RAY_TLS_SERVER_CERT\"] = os.path.join(\"./\", \"tls.crt\")\nos.environ[\"RAY_TLS_SERVER_KEY\"] = os.path.join(\"./\", \"tls.key\")\nos.environ[\"RAY_TLS_CA_CERT\"] = os.path.join(\"./\", \"ca.crt\")\nray.init(address=\"ray://localhost\", logging_level=\"DEBUG\")'\n</code></pre></p> <p>The output should be similar to: <pre><code>2023-04-25 16:33:32,452 INFO client_builder.py:253 -- Passing the following kwargs to ray.init() on the server: logging_level\n2023-04-25 16:33:32,460 DEBUG worker.py:378 -- client gRPC channel state change: ChannelConnectivity.IDLE\n2023-04-25 16:33:32,664 DEBUG worker.py:378 -- client gRPC channel state change: ChannelConnectivity.CONNECTING\n2023-04-25 16:33:32,671 DEBUG worker.py:378 -- client gRPC channel state change: ChannelConnectivity.READY\n</code></pre></p>"},{"location":"guidance/rayjob/","title":"RayJob","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/rayserve-dev-doc/","title":"Rayserve dev doc","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/rayservice-troubleshooting/","title":"RayService Troubleshooting","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/rayservice/","title":"RayService","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/stable-diffusion-rayservice/","title":"Stable diffusion rayservice","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/text-summarizer-rayservice/","title":"Text summarizer rayservice","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/tls/","title":"TLS","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"guidance/volcano-integration/","title":"KubeRay with Volcano","text":"<p>This document has been moved to the Ray documentation.</p>"},{"location":"reference/api/","title":"API Reference","text":""},{"location":"reference/api/#packages","title":"Packages","text":"<ul> <li>ray.io/v1</li> <li>ray.io/v1alpha1</li> </ul>"},{"location":"reference/api/#rayiov1","title":"ray.io/v1","text":"<p>Package v1 contains API Schema definitions for the ray v1 API group</p>"},{"location":"reference/api/#resource-types","title":"Resource Types","text":"<ul> <li>RayCluster</li> <li>RayJob</li> <li>RayService</li> </ul>"},{"location":"reference/api/#autoscaleroptions","title":"AutoscalerOptions","text":"<p>AutoscalerOptions specifies optional configuration for the Ray autoscaler.</p> <p>Appears in: - RayClusterSpec</p> Field Description <code>resources</code> ResourceRequirements Resources specifies optional resource request and limit overrides for the autoscaler container. Default values: 500m CPU request and limit. 512Mi memory request and limit. <code>image</code> string Image optionally overrides the autoscaler's container image. This override is for provided for autoscaler testing and development. <code>imagePullPolicy</code> PullPolicy ImagePullPolicy optionally overrides the autoscaler container's image pull policy. This override is for provided for autoscaler testing and development. <code>env</code> EnvVar array Optional list of environment variables to set in the autoscaler container. <code>envFrom</code> EnvFromSource array Optional list of sources to populate environment variables in the autoscaler container. <code>volumeMounts</code> VolumeMount array Optional list of volumeMounts.  This is needed for enabling TLS for the autoscaler container. <code>securityContext</code> SecurityContext SecurityContext defines the security options the container should be run with. If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext. More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ <code>idleTimeoutSeconds</code> integer IdleTimeoutSeconds is the number of seconds to wait before scaling down a worker pod which is not using Ray resources. Defaults to 60 (one minute). It is not read by the KubeRay operator but by the Ray autoscaler. <code>upscalingMode</code> UpscalingMode UpscalingMode is \"Conservative\", \"Default\", or \"Aggressive.\" Conservative: Upscaling is rate-limited; the number of pending worker pods is at most the size of the Ray cluster. Default: Upscaling is not rate-limited. Aggressive: An alias for Default; upscaling is not rate-limited. It is not read by the KubeRay operator but by the Ray autoscaler."},{"location":"reference/api/#headgroupspec","title":"HeadGroupSpec","text":"<p>HeadGroupSpec are the spec for the head pod</p> <p>Appears in: - RayClusterSpec</p> Field Description <code>serviceType</code> ServiceType ServiceType is Kubernetes service type of the head service. it will be used by the workers to connect to the head pod <code>headService</code> Service HeadService is the Kubernetes service of the head pod. <code>enableIngress</code> boolean EnableIngress indicates whether operator should create ingress object for head service or not. <code>rayStartParams</code> object (keys:string, values:string) RayStartParams are the params of the start command: node-manager-port, object-store-memory, ... <code>template</code> PodTemplateSpec Template is the exact pod template used in K8s depoyments, statefulsets, etc."},{"location":"reference/api/#jobfailedreason","title":"JobFailedReason","text":"<p>Underlying type: string</p> <p>JobFailedReason indicates the reason the RayJob changes its JobDeploymentStatus to 'Failed'</p> <p>Appears in: - RayJobStatus</p>"},{"location":"reference/api/#jobsubmissionmode","title":"JobSubmissionMode","text":"<p>Underlying type: string</p> <p>Appears in: - RayJobSpec</p>"},{"location":"reference/api/#raycluster","title":"RayCluster","text":"<p>RayCluster is the Schema for the RayClusters API</p> Field Description <code>apiVersion</code> string <code>ray.io/v1</code> <code>kind</code> string <code>RayCluster</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> RayClusterSpec Specification of the desired behavior of the RayCluster."},{"location":"reference/api/#rayclusterspec","title":"RayClusterSpec","text":"<p>RayClusterSpec defines the desired state of RayCluster</p> <p>Appears in: - RayCluster - RayJobSpec - RayServiceSpec</p> Field Description <code>headGroupSpec</code> HeadGroupSpec INSERT ADDITIONAL SPEC FIELDS - desired state of cluster Important: Run \"make\" to regenerate code after modifying this file HeadGroupSpecs are the spec for the head pod <code>workerGroupSpecs</code> WorkerGroupSpec array WorkerGroupSpecs are the specs for the worker pods <code>rayVersion</code> string RayVersion is used to determine the command for the Kubernetes Job managed by RayJob <code>enableInTreeAutoscaling</code> boolean EnableInTreeAutoscaling indicates whether operator should create in tree autoscaling configs <code>autoscalerOptions</code> AutoscalerOptions AutoscalerOptions specifies optional configuration for the Ray autoscaler. <code>headServiceAnnotations</code> object (keys:string, values:string) <code>suspend</code> boolean Suspend indicates whether a RayCluster should be suspended. A suspended RayCluster will have head pods and worker pods deleted."},{"location":"reference/api/#rayjob","title":"RayJob","text":"<p>RayJob is the Schema for the rayjobs API</p> Field Description <code>apiVersion</code> string <code>ray.io/v1</code> <code>kind</code> string <code>RayJob</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> RayJobSpec"},{"location":"reference/api/#rayjobspec","title":"RayJobSpec","text":"<p>RayJobSpec defines the desired state of RayJob</p> <p>Appears in: - RayJob</p> Field Description <code>entrypoint</code> string INSERT ADDITIONAL SPEC FIELDS - desired state of cluster Important: Run \"make\" to regenerate code after modifying this file <code>metadata</code> object (keys:string, values:string) Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>runtimeEnvYAML</code> string RuntimeEnvYAML represents the runtime environment configuration provided as a multi-line YAML string. <code>jobId</code> string If jobId is not set, a new jobId will be auto-generated. <code>shutdownAfterJobFinishes</code> boolean ShutdownAfterJobFinishes will determine whether to delete the ray cluster once rayJob succeed or failed. <code>ttlSecondsAfterFinished</code> integer TTLSecondsAfterFinished is the TTL to clean up RayCluster. It's only working when ShutdownAfterJobFinishes set to true. <code>activeDeadlineSeconds</code> integer ActiveDeadlineSeconds is the duration in seconds that the RayJob may be active before KubeRay actively tries to terminate the RayJob; value must be positive integer. <code>rayClusterSpec</code> RayClusterSpec RayClusterSpec is the cluster template to run the job <code>clusterSelector</code> object (keys:string, values:string) clusterSelector is used to select running rayclusters by labels <code>submissionMode</code> JobSubmissionMode SubmissionMode specifies how RayJob submits the Ray job to the RayCluster. In \"K8sJobMode\", the KubeRay operator creates a submitter Kubernetes Job to submit the Ray job. In \"HTTPMode\", the KubeRay operator sends a request to the RayCluster to create a Ray job. <code>suspend</code> boolean suspend specifies whether the RayJob controller should create a RayCluster instance If a job is applied with the suspend field set to true, the RayCluster will not be created and will wait for the transition to false. If the RayCluster is already created, it will be deleted. In case of transition to false a new RayCluster will be created. <code>submitterPodTemplate</code> PodTemplateSpec SubmitterPodTemplate is the template for the pod that will run <code>ray job submit</code>. <code>entrypointNumCpus</code> float EntrypointNumCpus specifies the number of cpus to reserve for the entrypoint command. <code>entrypointNumGpus</code> float EntrypointNumGpus specifies the number of gpus to reserve for the entrypoint command. <code>entrypointResources</code> string EntrypointResources specifies the custom resources and quantities to reserve for the entrypoint command."},{"location":"reference/api/#rayservice","title":"RayService","text":"<p>RayService is the Schema for the rayservices API</p> Field Description <code>apiVersion</code> string <code>ray.io/v1</code> <code>kind</code> string <code>RayService</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> RayServiceSpec"},{"location":"reference/api/#rayservicespec","title":"RayServiceSpec","text":"<p>RayServiceSpec defines the desired state of RayService</p> <p>Appears in: - RayService</p> Field Description <code>serveConfigV2</code> string Important: Run \"make\" to regenerate code after modifying this file Defines the applications and deployments to deploy, should be a YAML multi-line scalar string. <code>rayClusterConfig</code> RayClusterSpec <code>serviceUnhealthySecondThreshold</code> integer Deprecated: This field is not used anymore. ref: https://github.com/ray-project/kuberay/issues/1685 <code>deploymentUnhealthySecondThreshold</code> integer Deprecated: This field is not used anymore. ref: https://github.com/ray-project/kuberay/issues/1685 <code>serveService</code> Service ServeService is the Kubernetes service for head node and worker nodes who have healthy http proxy to serve traffics."},{"location":"reference/api/#scalestrategy","title":"ScaleStrategy","text":"<p>ScaleStrategy to remove workers</p> <p>Appears in: - WorkerGroupSpec</p> Field Description <code>workersToDelete</code> string array WorkersToDelete workers to be deleted"},{"location":"reference/api/#upscalingmode","title":"UpscalingMode","text":"<p>Underlying type: string</p> <p>Appears in: - AutoscalerOptions</p>"},{"location":"reference/api/#workergroupspec","title":"WorkerGroupSpec","text":"<p>WorkerGroupSpec are the specs for the worker pods</p> <p>Appears in: - RayClusterSpec</p> Field Description <code>groupName</code> string we can have multiple worker groups, we distinguish them by name <code>replicas</code> integer Replicas is the number of desired Pods for this worker group. See https://github.com/ray-project/kuberay/pull/1443 for more details about the reason for making this field optional. <code>minReplicas</code> integer MinReplicas denotes the minimum number of desired Pods for this worker group. <code>maxReplicas</code> integer MaxReplicas denotes the maximum number of desired Pods for this worker group, and the default value is maxInt32. <code>numOfHosts</code> integer NumOfHosts denotes the number of hosts to create per replica. The default value is 1. <code>rayStartParams</code> object (keys:string, values:string) RayStartParams are the params of the start command: address, object-store-memory, ... <code>template</code> PodTemplateSpec Template is a pod template for the worker <code>scaleStrategy</code> ScaleStrategy ScaleStrategy defines which pods to remove"},{"location":"reference/api/#rayiov1alpha1","title":"ray.io/v1alpha1","text":"<p>Package v1alpha1 contains API Schema definitions for the ray v1alpha1 API group</p>"},{"location":"reference/api/#resource-types_1","title":"Resource Types","text":"<ul> <li>RayCluster</li> <li>RayJob</li> <li>RayService</li> </ul>"},{"location":"reference/api/#autoscaleroptions_1","title":"AutoscalerOptions","text":"<p>AutoscalerOptions specifies optional configuration for the Ray autoscaler.</p> <p>Appears in: - RayClusterSpec</p> Field Description <code>resources</code> ResourceRequirements Resources specifies optional resource request and limit overrides for the autoscaler container. Default values: 500m CPU request and limit. 512Mi memory request and limit. <code>image</code> string Image optionally overrides the autoscaler's container image. This override is for provided for autoscaler testing and development. <code>imagePullPolicy</code> PullPolicy ImagePullPolicy optionally overrides the autoscaler container's image pull policy. This override is for provided for autoscaler testing and development. <code>env</code> EnvVar array Optional list of environment variables to set in the autoscaler container. <code>envFrom</code> EnvFromSource array Optional list of sources to populate environment variables in the autoscaler container. <code>volumeMounts</code> VolumeMount array Optional list of volumeMounts.  This is needed for enabling TLS for the autoscaler container. <code>securityContext</code> SecurityContext SecurityContext defines the security options the container should be run with. If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext. More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ <code>idleTimeoutSeconds</code> integer IdleTimeoutSeconds is the number of seconds to wait before scaling down a worker pod which is not using Ray resources. Defaults to 60 (one minute). It is not read by the KubeRay operator but by the Ray autoscaler. <code>upscalingMode</code> UpscalingMode UpscalingMode is \"Conservative\", \"Default\", or \"Aggressive.\" Conservative: Upscaling is rate-limited; the number of pending worker pods is at most the size of the Ray cluster. Default: Upscaling is not rate-limited. Aggressive: An alias for Default; upscaling is not rate-limited. It is not read by the KubeRay operator but by the Ray autoscaler."},{"location":"reference/api/#headgroupspec_1","title":"HeadGroupSpec","text":"<p>HeadGroupSpec are the spec for the head pod</p> <p>Appears in: - RayClusterSpec</p> Field Description <code>serviceType</code> ServiceType ServiceType is Kubernetes service type of the head service. it will be used by the workers to connect to the head pod <code>headService</code> Service HeadService is the Kubernetes service of the head pod. <code>enableIngress</code> boolean EnableIngress indicates whether operator should create ingress object for head service or not. <code>rayStartParams</code> object (keys:string, values:string) RayStartParams are the params of the start command: node-manager-port, object-store-memory, ... <code>template</code> PodTemplateSpec Template is the exact pod template used in K8s depoyments, statefulsets, etc."},{"location":"reference/api/#raycluster_1","title":"RayCluster","text":"<p>RayCluster is the Schema for the RayClusters API</p> Field Description <code>apiVersion</code> string <code>ray.io/v1alpha1</code> <code>kind</code> string <code>RayCluster</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> RayClusterSpec Specification of the desired behavior of the RayCluster."},{"location":"reference/api/#rayclusterspec_1","title":"RayClusterSpec","text":"<p>RayClusterSpec defines the desired state of RayCluster</p> <p>Appears in: - RayCluster - RayJobSpec - RayServiceSpec</p> Field Description <code>headGroupSpec</code> HeadGroupSpec INSERT ADDITIONAL SPEC FIELDS - desired state of cluster Important: Run \"make\" to regenerate code after modifying this file HeadGroupSpecs are the spec for the head pod <code>workerGroupSpecs</code> WorkerGroupSpec array WorkerGroupSpecs are the specs for the worker pods <code>rayVersion</code> string RayVersion is used to determine the command for the Kubernetes Job managed by RayJob <code>enableInTreeAutoscaling</code> boolean EnableInTreeAutoscaling indicates whether operator should create in tree autoscaling configs <code>autoscalerOptions</code> AutoscalerOptions AutoscalerOptions specifies optional configuration for the Ray autoscaler. <code>headServiceAnnotations</code> object (keys:string, values:string) <code>suspend</code> boolean Suspend indicates whether a RayCluster should be suspended. A suspended RayCluster will have head pods and worker pods deleted."},{"location":"reference/api/#rayjob_1","title":"RayJob","text":"<p>RayJob is the Schema for the rayjobs API</p> Field Description <code>apiVersion</code> string <code>ray.io/v1alpha1</code> <code>kind</code> string <code>RayJob</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> RayJobSpec"},{"location":"reference/api/#rayjobspec_1","title":"RayJobSpec","text":"<p>RayJobSpec defines the desired state of RayJob</p> <p>Appears in: - RayJob</p> Field Description <code>entrypoint</code> string INSERT ADDITIONAL SPEC FIELDS - desired state of cluster Important: Run \"make\" to regenerate code after modifying this file <code>metadata</code> object (keys:string, values:string) Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>runtimeEnvYAML</code> string RuntimeEnvYAML represents the runtime environment configuration provided as a multi-line YAML string. <code>jobId</code> string If jobId is not set, a new jobId will be auto-generated. <code>shutdownAfterJobFinishes</code> boolean ShutdownAfterJobFinishes will determine whether to delete the ray cluster once rayJob succeed or failed. <code>ttlSecondsAfterFinished</code> integer TTLSecondsAfterFinished is the TTL to clean up RayCluster. It's only working when ShutdownAfterJobFinishes set to true. <code>rayClusterSpec</code> RayClusterSpec RayClusterSpec is the cluster template to run the job <code>clusterSelector</code> object (keys:string, values:string) clusterSelector is used to select running rayclusters by labels <code>suspend</code> boolean suspend specifies whether the RayJob controller should create a RayCluster instance If a job is applied with the suspend field set to true, the RayCluster will not be created and will wait for the transition to false. If the RayCluster is already created, it will be deleted. In case of transition to false a new RayCluster will be created. <code>submitterPodTemplate</code> PodTemplateSpec SubmitterPodTemplate is the template for the pod that will run <code>ray job submit</code>. <code>entrypointNumCpus</code> float EntrypointNumCpus specifies the number of cpus to reserve for the entrypoint command. <code>entrypointNumGpus</code> float EntrypointNumGpus specifies the number of gpus to reserve for the entrypoint command. <code>entrypointResources</code> string EntrypointResources specifies the custom resources and quantities to reserve for the entrypoint command."},{"location":"reference/api/#rayservice_1","title":"RayService","text":"<p>RayService is the Schema for the rayservices API</p> Field Description <code>apiVersion</code> string <code>ray.io/v1alpha1</code> <code>kind</code> string <code>RayService</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> RayServiceSpec"},{"location":"reference/api/#rayservicespec_1","title":"RayServiceSpec","text":"<p>RayServiceSpec defines the desired state of RayService</p> <p>Appears in: - RayService</p> Field Description <code>serveConfigV2</code> string Important: Run \"make\" to regenerate code after modifying this file Defines the applications and deployments to deploy, should be a YAML multi-line scalar string. <code>rayClusterConfig</code> RayClusterSpec <code>serviceUnhealthySecondThreshold</code> integer Deprecated: This field is not used anymore. ref: https://github.com/ray-project/kuberay/issues/1685 <code>deploymentUnhealthySecondThreshold</code> integer Deprecated: This field is not used anymore. ref: https://github.com/ray-project/kuberay/issues/1685 <code>serveService</code> Service ServeService is the Kubernetes service for head node and worker nodes who have healthy http proxy to serve traffics."},{"location":"reference/api/#scalestrategy_1","title":"ScaleStrategy","text":"<p>ScaleStrategy to remove workers</p> <p>Appears in: - WorkerGroupSpec</p> Field Description <code>workersToDelete</code> string array WorkersToDelete workers to be deleted"},{"location":"reference/api/#upscalingmode_1","title":"UpscalingMode","text":"<p>Underlying type: string</p> <p>Appears in: - AutoscalerOptions</p>"},{"location":"reference/api/#workergroupspec_1","title":"WorkerGroupSpec","text":"<p>WorkerGroupSpec are the specs for the worker pods</p> <p>Appears in: - RayClusterSpec</p> Field Description <code>groupName</code> string we can have multiple worker groups, we distinguish them by name <code>replicas</code> integer Replicas is the number of desired Pods for this worker group. See https://github.com/ray-project/kuberay/pull/1443 for more details about the reason for making this field optional. <code>minReplicas</code> integer MinReplicas denotes the minimum number of desired Pods for this worker group. <code>maxReplicas</code> integer MaxReplicas denotes the maximum number of desired Pods for this worker group, and the default value is maxInt32. <code>rayStartParams</code> object (keys:string, values:string) RayStartParams are the params of the start command: address, object-store-memory, ... <code>template</code> PodTemplateSpec Template is a pod template for the worker <code>scaleStrategy</code> ScaleStrategy ScaleStrategy defines which pods to remove"},{"location":"release/changelog/","title":"Generate the changelog for a release","text":""},{"location":"release/changelog/#prerequisite","title":"Prerequisite","text":"<ol> <li> <p>Prepare your Github Token</p> </li> <li> <p>Install the Github python dependencies needed to generate the changelog.     <pre><code>pip install PyGithub\n</code></pre></p> </li> </ol>"},{"location":"release/changelog/#generate-release-notes","title":"Generate release notes","text":"<ol> <li> <p>Run the following command and fetch oneline git commits from the last release (v0.3.0) to current release (v0.4.0).</p> <pre><code>git log v0.3.0..v0.4.0 --oneline\n</code></pre> <p>You may need to run the following command first:</p> <pre><code>git fetch --tags\n</code></pre> </li> <li> <p>Copy the above commit history to <code>scripts/changelog-generator.py</code> and replace <code>&lt;your_github_token&gt;</code> with your Github token. Run the script to generate changelogs.</p> <pre><code>from github import Github\nimport re\n\n\nclass ChangelogGenerator:\n    def __init__(self, github_repo):\n        # Replace &lt;your_github_token&gt; with your Github Token\n        self._github = Github('&lt;your_github_token&gt;')\n        self._github_repo = self._github.get_repo(github_repo)\n\n    def generate(self, pr_id):\n        pr = self._github_repo.get_pull(pr_id)\n\n        return \"{title} ([#{pr_id}]({pr_link}), @{user})\".format(\n            title=pr.title,\n            pr_id=pr_id,\n            pr_link=pr.html_url,\n            user=pr.user.login\n        )\n\n\n# generated by `git log &lt;oldTag&gt;..&lt;newTag&gt; --oneline`\npayload = '''\n7374e2c [RayService] Skip update events without change (#811) (#825)\n7f83353 Switch to 0.4.0 and eliminate Chart app versions. (#810)\n86b0af2 Remove ingress.enabled from KubeRay operator chart (#812) (#816)\nc1cbaed Update chart versions for 0.4.0-rc.0 (#804)\n84a70f1 Update image tags. (#784)\nd760b9c [helm] Add memory limits and resource documentation. (#789) (#798)\n16905df [Feature] Improve the observability of integration tests (#775) (#796)\n83aab82 [CI] Pin go version in CRD consistency check (#794) (#797)\n....\n'''\n\ng = ChangelogGenerator(\"ray-project/kuberay\")\nfor pr_match in re.finditer(r\"#(\\d+)\", payload):\n    pr_id = int(pr_match.group(1))\n    print(\"* {}\".format(g.generate(pr_id)))\n</code></pre> </li> <li> <p>To create the release notes, save the output of the script. Modify the script's output as follows.</p> <ul> <li>Remove extraneous data, such as commits with tag information or links to other PRs, e.g. <pre><code>- c1cbaed (tag: v0.4.0-rc.0) Update chart versions for 0.4.0-rc.0 (#804) -&gt; c1cbaed Update chart versions for 0.4.0-rc.0 (#804)\n- 86b0af2 Remove ingress.enabled from KubeRay operator chart (#812) (#816) -&gt; 86b0af2 Remove ingress.enabled from KubeRay operator chart (#816)\n</code></pre></li> <li>Group commits by category e.g. <code>KubeRay Operator</code>, <code>Documentation</code>, etc. (The choice of categories is at the release manager's discretion.)</li> <li>Add a section summarizing important changes.</li> <li>Add a section listing individuals who contributed to the release.</li> </ul> </li> <li> <p>Cut the release from tags and add the release notes from the last step. For an example, see the v0.3.0 release notes.</p> </li> <li> <p>Send a PR to update CHANGELOG.md. The changelog should be updated by prepending the new release notes.</p> </li> </ol>"},{"location":"release/helm-chart/","title":"Helm charts release","text":"<p>We host all Helm charts on kuberay-helm. This document describes the process for release managers to release Helm charts.</p>"},{"location":"release/helm-chart/#the-end-to-end-workflow","title":"The end-to-end workflow","text":""},{"location":"release/helm-chart/#step-1-update-versions-in-chartyaml-and-valuesyaml-files","title":"Step 1: Update versions in Chart.yaml and values.yaml files","text":"<p>Please update the value of <code>version</code> in ray-cluster/Chart.yaml, kuberay-operator/Chart.yaml, and kuberay-apiserver/Chart.yaml to the new release version (e.g. 0.4.0).</p> <p>Also make sure <code>image.tag</code> has been updated in kuberay-operator/values.yaml and kuberay-apiserver/values.yaml.</p>"},{"location":"release/helm-chart/#step-2-copy-the-helm-chart-directory-from-kuberay-to-kuberay-helm","title":"Step 2: Copy the helm-chart directory from kuberay to kuberay-helm","text":"<p>In kuberay-helm CI, <code>helm/chart-releaser-action</code> will create releases for all charts in the directory <code>helm-chart</code> and update <code>index.yaml</code> in the gh-pages branch when the PR is merged into <code>main</code>. Note that <code>index.yaml</code> is necessary when you run the command <code>helm repo add</code>. I recommend removing the <code>helm-chart</code> directory in the kuberay-helm repository and creating a new one by copying from the kuberay repository.</p>"},{"location":"release/helm-chart/#step-3-validate-the-charts","title":"Step 3: Validate the charts","text":"<p>When the PR is merged into <code>main</code>, the releases and <code>index.yaml</code> will be generated. You can validate the charts as follows:</p> <ul> <li>Confirm that the releases are created as expected.</li> <li>Confirm that index.yaml exists.</li> <li>Confirm that index.yaml has the metadata of all releases, including old versions.</li> <li> <p>Check the creation/update time of all releases and <code>index.yaml</code> to ensure they are updated.</p> </li> <li> <p>Install charts from Helm repository.     <pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm repo update\n\n# List all charts\nhelm search repo kuberay\n\n# Install charts\nhelm install kuberay-operator kuberay/kuberay-operator\nhelm install kuberay-apiserver kuberay/kuberay-apiserver\nhelm install raycluster kuberay/ray-cluster\n</code></pre></p> </li> </ul>"},{"location":"release/helm-chart/#delete-the-existing-releases","title":"Delete the existing releases","text":"<p><code>helm/chart-releaser-action</code> does not encourage users to delete existing releases; thus, <code>index.yaml</code> will not be updated automatically after the deletion. If you really need to do that, please read this section carefully before you do that.</p> <ul> <li>Delete the releases</li> <li> <p>Remove the related tags using the the following command. If tags are not properly removed, you may run into the problem described in ray-project/kuberay/#561.</p> <p><pre><code># git remote -v\n# upstream        git@github.com:ray-project/kuberay-helm.git (fetch)\n# upstream        git@github.com:ray-project/kuberay-helm.git (push)\n\n# The following command deletes the tag \"ray-cluster-0.4.0\".\ngit push --delete upstream ray-cluster-0.4.0\n</code></pre> * Remove <code>index.yaml</code> * Trigger kuberay-helm CI again to create new releases and a new index.yaml. * Follow \"Step 3: Validate the charts\" to test it.</p> </li> </ul>"}]}